Using device: cpu                                                                                                                                             CPU cores available: 32
CPU cores using: 24
Data root directory: /home/krschap/rabina/data/s2a
========================
Dataset config: DataConfig(data_root_dir='/home/krschap/rabina/data/s2a', compute_stats=False, n_samples=None, batch_size=64, patch_size=264, num_workers=24)
========================
Training config: TrainingConfig(experiment_out_dir='output/ssl_v1_e20_b96_mem_16k', model='resnet50', in_channels=13, version=2, lr=0.0001, use_peft=False, temperature=0.15, memory_bank_size=16000, target_size=224, max_epochs=20, batch_size=96)
Using pre-computed mean and std
166775
torch.Size([13, 224, 224])
Number of batches: 1738
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/torchgeo/trainers/moco.py:209: UserWarning: MoCo v2 only uses 2 layers in its projection head
  warnings.warn('MoCo v2 only uses 2 layers in its projection head')
Full fine-tuning: backbone and projection head trainable...
+--------------------------+------------------------+------------------+--------------+
|          Module          |          Type          | Trainable Params | Total Params |
+--------------------------+------------------------+------------------+--------------+
|         backbone         |         ResNet         |    23,539,392    |  23,539,392  |
|    backbone_momentum     |         ResNet         |        0         |  23,539,392  |
|     projection_head      |   MoCoProjectionHead   |    26,222,848    |  26,222,848  |
| projection_head_momentum |   MoCoProjectionHead   |        0         |  26,222,848  |
|        criterion         |       NTXentLoss       |        0         |      0       |
|      augmentation1       | AugmentationSequential |        0         |      0       |
+--------------------------+------------------------+------------------+--------------+
Total trainable parameters: 49,762,240 (49.76 M)
Total parameters: 99,524,480 (99.52 M)
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
� Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.
����������������������������������������������������������������������������������
�   � Name                     � Type                   � Params � Mode  � FLOPs �
����������������������������������������������������������������������������������
� 0 � backbone                 � ResNet                 � 23.5 M � train �     0 �
� 1 � backbone_momentum        � ResNet                 � 23.5 M � train �     0 �
� 2 � projection_head          � MoCoProjectionHead     � 26.2 M � train �     0 �
� 3 � projection_head_momentum � MoCoProjectionHead     � 26.2 M � train �     0 �
� 4 � criterion                � NTXentLoss             �      0 � train �     0 �
� 5 � augmentation1            � AugmentationSequential �      0 � train �     0 �
����������������������������������������������������������������������������������
Trainable params: 49.8 M
Non-trainable params: 49.8 M
Total params: 99.5 M
Total estimated model params size (MB): 398
Modules in train mode: 460
Modules in eval mode: 0
Total FLOPs: 0
Epoch 19/19 ???????????????????????????????????????? 1738/1738 0:09:28 ? 0:00:00 3.16it/s v_num: 0.000`Trainer.fit` stopped: `max_epochs=20` reached.
Epoch 19/19 ???????????????????????????????????????? 1738/1738 0:09:28 ? 0:00:00 3.16it/s v_num: 0.000
Training time: 189.60706681013107 min
`weights_only` was not set, defaulting to `False`.




/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/torchgeo/trainers/moco.py:209: UserWarning: MoCo v2 only uses 2 layers in its projection head
  warnings.warn('MoCo v2 only uses 2 layers in its projection head')
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
? Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_b96_mem_16k/ssl_ckpt_20260215_042511.ckpt
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:566: The dirpath has changed from '/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_b96_mem_16k' to '/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_50_b96_mem_16k', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.
Using device: cpu
CPU cores available: 32
CPU cores using: 24
Data root directory: /home/krschap/rabina/data/s2a
========================
Dataset config: DataConfig(data_root_dir='/home/krschap/rabina/data/s2a', compute_stats=False, n_samples=None, batch_size=64, patch_size=264, num_workers=24)
========================
Training config: TrainingConfig(experiment_out_dir='output/ssl_v1_e20_50_b96_mem_16k', model='resnet50', in_channels=13, version=2, lr=0.0001, use_peft=False, temperature=0.15, memory_bank_size=16000, target_size=224, max_epochs=50, batch_size=96, ckpt_path='/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_b96_mem_16k/ssl_ckpt_20260215_042511.ckpt')
Using pre-computed mean and std
166775
torch.Size([13, 224, 224])
Number of batches: 1738
Full fine-tuning: backbone and projection head trainable...
+--------------------------+------------------------+------------------+--------------+
|          Module          |          Type          | Trainable Params | Total Params |
+--------------------------+------------------------+------------------+--------------+
|         backbone         |         ResNet         |    23,539,392    |  23,539,392  |
|    backbone_momentum     |         ResNet         |        0         |  23,539,392  |
|     projection_head      |   MoCoProjectionHead   |    26,222,848    |  26,222,848  |
| projection_head_momentum |   MoCoProjectionHead   |        0         |  26,222,848  |
|        criterion         |       NTXentLoss       |        0         |      0       |
|      augmentation1       | AugmentationSequential |        0         |      0       |
+--------------------------+------------------------+------------------+--------------+
Total trainable parameters: 49,762,240 (49.76 M)
Total parameters: 99,524,480 (99.52 M)
??????????????????????????????????????????????????????????????????????????????????
?   ? Name                     ? Type                   ? Params ? Mode  ? FLOPs ?
??????????????????????????????????????????????????????????????????????????????????
? 0 ? backbone                 ? ResNet                 ? 23.5 M ? train ?     0 ?
? 1 ? backbone_momentum        ? ResNet                 ? 23.5 M ? train ?     0 ?
? 2 ? projection_head          ? MoCoProjectionHead     ? 26.2 M ? train ?     0 ?
? 3 ? projection_head_momentum ? MoCoProjectionHead     ? 26.2 M ? train ?     0 ?
? 4 ? criterion                ? NTXentLoss             ?      0 ? train ?     0 ?
? 5 ? augmentation1            ? AugmentationSequential ?      0 ? train ?     0 ?
??????????????????????????????????????????????????????????????????????????????????
Trainable params: 49.8 M                                                                                                                                                                                                                          
Non-trainable params: 49.8 M                                                                                                                                                                                                                      
Total params: 99.5 M                                                                                                                                                                                                                              
Total estimated model params size (MB): 398                                                                                                                                                                                                       
Modules in train mode: 460                                                                                                                                                                                                                        
Modules in eval mode: 0                                                                                                                                                                                                                           
Total FLOPs: 0                                                                                                                                                                                                                                    
Restored all states from the checkpoint at /home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_b96_mem_16k/ssl_ckpt_20260215_042511.ckpt
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.

Epoch 49/49 ???????????????????????????????????????? 1738/1738 0:09:28 ? 0:00:00 3.14it/s v_num: 0.000`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ???????????????????????????????????????? 1738/1738 0:09:28 ? 0:00:00 3.14it/s v_num: 0.000
Training time: 284.8437604745229 min
`weights_only` was not set, defaulting to `False`.



Using device: cpu
CPU cores available: 32
CPU cores using: 24
Data root directory: /home/krschap/rabina/data/s2a
========================
Dataset config: DataConfig(data_root_dir='/home/krschap/rabina/data/s2a', compute_stats=False, n_samples=None, batch_size=64, patch_size=264, num_workers=24)
========================
Training config: TrainingConfig(experiment_out_dir='output/ssl_v2_e50_100_b96_mem_16k', model='resnet50', in_channels=13, version=2, lr=0.0001, use_peft=False, temperature=0.15, memory_bank_size=16000, target_size=224, max_epochs=100, batch_size=96, ckpt_path='/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_50_b96_mem_16k/ssl_ckpt_20260215_104921.ckpt')
Using pre-computed mean and std
166775
torch.Size([13, 224, 224])
Number of batches: 1738
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/torchgeo/trainers/moco.py:209: UserWarning: MoCo v2 only uses 2 layers in its projection head
  warnings.warn('MoCo v2 only uses 2 layers in its projection head')
Full fine-tuning: backbone and projection head trainable...
+--------------------------+------------------------+------------------+--------------+
|          Module          |          Type          | Trainable Params | Total Params |
+--------------------------+------------------------+------------------+--------------+
|         backbone         |         ResNet         |    23,539,392    |  23,539,392  |
|    backbone_momentum     |         ResNet         |        0         |  23,539,392  |
|     projection_head      |   MoCoProjectionHead   |    26,222,848    |  26,222,848  |
| projection_head_momentum |   MoCoProjectionHead   |        0         |  26,222,848  |
|        criterion         |       NTXentLoss       |        0         |      0       |
|      augmentation1       | AugmentationSequential |        0         |      0       |
+--------------------------+------------------------+------------------+--------------+
Total trainable parameters: 49,762,240 (49.76 M)
Total parameters: 99,524,480 (99.52 M)
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
? Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.
USING DEVICE CONFIRMATION cpu
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_50_b96_mem_16k/ssl_ckpt_20260215_104921.ckpt
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:566: The dirpath has changed from '/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_50_b96_mem_16k' to '/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v2_e50_100_b96_mem_16k', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.
??????????????????????????????????????????????????????????????????????????????????
?   ? Name                     ? Type                   ? Params ? Mode  ? FLOPs ?
??????????????????????????????????????????????????????????????????????????????????
? 0 ? backbone                 ? ResNet                 ? 23.5 M ? train ?     0 ?
? 1 ? backbone_momentum        ? ResNet                 ? 23.5 M ? train ?     0 ?
? 2 ? projection_head          ? MoCoProjectionHead     ? 26.2 M ? train ?     0 ?
? 3 ? projection_head_momentum ? MoCoProjectionHead     ? 26.2 M ? train ?     0 ?
? 4 ? criterion                ? NTXentLoss             ?      0 ? train ?     0 ?
? 5 ? augmentation1            ? AugmentationSequential ?      0 ? train ?     0 ?
??????????????????????????????????????????????????????????????????????????????????
Trainable params: 49.8 M                                                                                                                                                                                                                          
Non-trainable params: 49.8 M                                                                                                                                                                                                                      
Total params: 99.5 M                                                                                                                                                                                                                              
Total estimated model params size (MB): 398                                                                                                                                                                                                       
Modules in train mode: 460                                                                                                                                                                                                                        
Modules in eval mode: 0                                                                                                                                                                                                                           
Total FLOPs: 0                                                                                                                                                                                                                                    
Restored all states from the checkpoint at /home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_50_b96_mem_16k/ssl_ckpt_20260215_104921.ckpt
/home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.
Epoch 99/99 ââââââââââââââââââââââââââââââââââââââââ 1738/1738 0:09:27 â¢ 0:00:00 3.16it/s v_num: 0.000`Trainer.fit` stopped: `max_epochs=100` reached.
Epoch 99/99 ââââââââââââââââââââââââââââââââââââââââ 1738/1738 0:09:27 â¢ 0:00:00 3.16it/s v_num: 0.000
After fit device: cpu
Training time: 473.4684624314308 min
`weights_only` was not set, defaulting to `False`.



