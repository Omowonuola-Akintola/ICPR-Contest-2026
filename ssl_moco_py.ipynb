{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2aad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ========================\n",
    "# Imports\n",
    "# ========================\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from prettytable import PrettyTable\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torchvision import transforms\n",
    "#from torchvision.models import ResNet50_Weights\n",
    "from lightning.pytorch import Trainer\n",
    "import kornia.augmentation as K\n",
    "from dataclasses import dataclass\n",
    "from torchgeo.trainers.moco import MoCoTask\n",
    "from torchgeo.models.resnet import ResNet50_Weights\n",
    "import pytorch_lightning as pl\n",
    "from typing import Optional\n",
    "# ========================\n",
    "# GPU & CPU setup\n",
    "# ========================\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Reproducibility\n",
    "# ========================\n",
    "seed = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "pl.seed_everything(seed, workers=True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Timestamp for logging / checkpoints\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    data_root_dir: str = \"/home/krschap/rabina/data/s2a\"\n",
    "    compute_stats: bool = True\n",
    "    n_samples: int = None\n",
    "    batch_size: int = 64\n",
    "    patch_size: int = 264\n",
    "    num_workers: int = 1\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    experiment_out_dir: str = \"ssl_moco_\"\n",
    "    model: str = \"resnet50\"\n",
    "    #weights: ResNet50_Weights = ResNet50_Weights.SENTINEL2_ALL_MOCO\n",
    "    in_channels: int = 13\n",
    "    version: int = 2\n",
    "    lr: float = 1e-4\n",
    "    use_peft: bool = False\n",
    "    temperature: float = 0.15\n",
    "    memory_bank_size: int = 2048\n",
    "    target_size: int = 224\n",
    "    max_epochs: int = 100\n",
    "    batch_size: int =32\n",
    "    ckpt_path: Optional[str] = None\n",
    "    #devices = []\n",
    "\n",
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, scenes, bands, transforms=None, patch_size=264):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scenes (list): List of scene folder paths.\n",
    "            bands (list): List of band names (e.g., [\"B1\",\"B2\"]).\n",
    "            patch_size (tuple): Size of random crop (H, W).\n",
    "            transforms (callable, optional): Optional transform to apply to patches.\n",
    "        \"\"\"\n",
    "        self.scenes = scenes\n",
    "        self.bands = bands\n",
    "        self.patch_size = patch_size\n",
    "        self.transforms = transforms\n",
    "        # Precompute all timestamp paths to treat each timestamp as a sample\n",
    "        self.samples = []\n",
    "        for scene_path in scenes:\n",
    "            timestamps = sorted([\n",
    "                d for d in os.listdir(scene_path)\n",
    "                if os.path.isdir(os.path.join(scene_path, d))\n",
    "            ])\n",
    "            for ts in timestamps:\n",
    "                self.samples.append(os.path.join(scene_path, ts))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ts_path = self.samples[idx]\n",
    "\n",
    "        band_arrays = []\n",
    "\n",
    "        for b in self.bands:\n",
    "            path = os.path.join(ts_path, f\"{b}.tif\")\n",
    "            with rasterio.open(path) as src:\n",
    "                if src.height == self.patch_size and src.width == self.patch_size:\n",
    "                    arr = src.read(1).astype(np.float32)\n",
    "                else:\n",
    "                    arr = src.read(\n",
    "                        1,\n",
    "                        out_shape=(self.patch_size, self.patch_size),\n",
    "                        resampling=Resampling.bilinear\n",
    "                    ).astype(np.float32)\n",
    "\n",
    "            band_arrays.append(arr)\n",
    "\n",
    "        # Insert fake B10\n",
    "        insert_idx = 10\n",
    "        b10_pad = np.zeros((self.patch_size, self.patch_size), dtype=np.float32)\n",
    "        band_arrays.insert(insert_idx, b10_pad)\n",
    "\n",
    "        img = np.stack(band_arrays, axis=0)\n",
    "\n",
    "        # img_patch = self._random_crop(img)\n",
    "\n",
    "        patch_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "        if self.transforms:\n",
    "            patch_tensor = self.transforms(patch_tensor)\n",
    "\n",
    "        return {\"image\": patch_tensor}\n",
    "\n",
    "def calculate_stats_parallel(dataset, n_samples=None, batch_size=16, num_workers=4):\n",
    "\n",
    "    total = len(dataset)\n",
    "    print(f\"Total samples in dataset: {total}\")\n",
    "\n",
    "    if n_samples is not None:\n",
    "        n = min(total, n_samples)\n",
    "        print(f\"Calculating stats on {n} randomly selected samples...\")\n",
    "        # Randomly select a subset of indices for efficiency\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(total, size=n, replace=False)\n",
    "        subset = Subset(dataset, indices)\n",
    "    else:\n",
    "        print(f\"Calculating stats on the entire dataset...\")\n",
    "        subset = dataset\n",
    "        n_samples=total\n",
    "\n",
    "    loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    channel_sum = 0.0\n",
    "    channel_sum_sq = 0.0\n",
    "    num_pixels = 0\n",
    "\n",
    "    total_batches = len(loader)\n",
    "    print(f\"Total batches to process: {total_batches}\")\n",
    "    for batch_idx, batch in enumerate(loader, start=1):\n",
    "        imgs = batch[\"image\"]\n",
    "        b, c, h, w = imgs.shape\n",
    "        channel_sum += imgs.sum(dim=(0, 2, 3))\n",
    "        channel_sum_sq += (imgs**2).sum(dim=(0, 2, 3))\n",
    "        num_pixels += b * h * w\n",
    "\n",
    "        if batch_idx % 500 == 0 or batch_idx == total_batches:\n",
    "            print(f\"Processed batch {batch_idx}/{total_batches} \")\n",
    "                # f\"â‰ˆ {batch_idx * b}/{n_samples} samples\")\n",
    "\n",
    "    mean = channel_sum / num_pixels\n",
    "    torch.set_printoptions(sci_mode=False, precision=4)\n",
    "    # std  = torch.sqrt(channel_sum_sq / num_pixels - mean**2)\n",
    "    variance = channel_sum_sq / num_pixels - mean**2\n",
    "    variance = torch.clamp(variance, min=0)  # Avoid negative values\n",
    "    # Add small epsilon to avoid sqrt(0) issues\n",
    "    std = torch.sqrt(variance + 1e-8)\n",
    "    return mean, std\n",
    "\n",
    "def summary_trainable(model):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Module\", \"Type\", \"Trainable Params\", \"Total Params\"]\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        total_params = sum(p.numel() for p in module.parameters())\n",
    "        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        table.add_row([name, type(module).__name__, f\"{trainable_params:,}\", f\"{total_params:,}\"])\n",
    "\n",
    "    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(table)\n",
    "    print(f\"Total trainable parameters: {total_trainable:,} ({total_trainable / 1e6:.2f} M)\")\n",
    "    print(f\"Total parameters: {total_params:,} ({total_params / 1e6:.2f} M)\")\n",
    "\n",
    "\n",
    "# def main(data_root_dir, n_samples,  batch_size, patch_size, num_workers):\n",
    "def main(data_cfg, training_cfg):\n",
    "    print(\"Data root directory:\", data_cfg.data_root_dir)\n",
    "    print(\"========================\")\n",
    "    print(\"Dataset config:\", data_cfg)\n",
    "    print(\"========================\")\n",
    "    print(\"Training config:\", training_cfg)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    os.makedirs(training_cfg.experiment_out_dir, exist_ok=True)\n",
    "    logger = CSVLogger(\"logs\", name=f\"{training_cfg.experiment_out_dir}/metrics_{timestamp}\")\n",
    "\n",
    "    aug = K.AugmentationSequential(\n",
    "        K.RandomResizedCrop(size=(training_cfg.target_size, training_cfg.target_size), scale=(0.4, 1.0)),\n",
    "        K.RandomHorizontalFlip(),\n",
    "        K.RandomVerticalFlip(),\n",
    "        K.RandomGaussianBlur(kernel_size=(7,7), sigma=(0.1, 1.5), p=0.3),\n",
    "        K.RandomBrightness(brightness=(0.85, 1.15), p=0.5),\n",
    "        data_keys=['input'],\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(data_cfg.data_root_dir):\n",
    "        raise FileNotFoundError(f\"Data root directory does not exist: {data_cfg.data_root_dir}\")\n",
    "    scenes = sorted(glob.glob(os.path.join(data_cfg.data_root_dir, \"*/\")))\n",
    "    bands = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"B11\",\"B12\"]\n",
    "    # ========================\n",
    "    # Compute dataset statistics (mean, std)\n",
    "    # ========================\n",
    "    if data_cfg.compute_stats:\n",
    "        start_time = time.time()\n",
    "        #scenes = sorted(glob.glob(os.path.join(data_cfg.data_root_dir, \"*/\")))\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Found {len(scenes)} scenes in {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "        # bands = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"B11\",\"B12\"]\n",
    "        temp_dataset = SSLDataset(scenes, bands, patch_size=data_cfg.patch_size)\n",
    "        start_time = time.time()\n",
    "        # mean, std = calculate_stats(temp_dataset, n_samples=10000)\n",
    "        mean, std = calculate_stats_parallel(temp_dataset, n_samples=data_cfg.n_samples, batch_size=data_cfg.batch_size, num_workers=data_cfg.num_workers)\n",
    "        end_time = time.time()\n",
    "        print(f\"calculate_stats time: {(end_time-start_time)/60:.2f} min\")\n",
    "        print(\"Mean:\", mean)\n",
    "        print(\"Std:\", std)\n",
    "        mean = mean.tolist()\n",
    "        std = std.tolist()\n",
    "    else:\n",
    "        print(\"Using pre-computed mean and std\")\n",
    "        mean =[1278.6852, 1434.4397, 1694.8578, 1932.4095, 2269.2026, 2789.2571,\n",
    "            3019.3687, 3144.4609, 3180.2004, 3289.7437,    0.0000, 2666.2808,\n",
    "            2098.4189]\n",
    "        \n",
    "        std= [2148.8049, 2148.8838, 2038.1532, 2097.1721, 2066.5100, 1890.2559,      \n",
    "            1842.6375, 1898.1923, 1762.8573, 1992.9287,    0.0001, 1247.9540,\n",
    "            1132.2416]\n",
    "\n",
    "    # ========================\n",
    "    # Train MoCo model\n",
    "    # ========================\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((training_cfg.target_size, training_cfg.target_size)),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "        \n",
    "    dataset = SSLDataset(scenes, bands, transforms=transform, patch_size=training_cfg.target_size)\n",
    "    print(len(dataset))\n",
    "    print(dataset[0]['image'].shape)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=training_cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=data_cfg.num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "    num_batches = len(data_loader)\n",
    "    print(\"Number of batches:\", num_batches)\n",
    "\n",
    "    import time\n",
    "    task = MoCoTask(\n",
    "        model=training_cfg.model,      \n",
    "        weights= ResNet50_Weights.SENTINEL2_ALL_MOCO,\n",
    "        in_channels=training_cfg.in_channels,       \n",
    "        version=training_cfg.version,             # MoCo v2\n",
    "        size=training_cfg.target_size,          \n",
    "        augmentation1=aug,\n",
    "        augmentation2=aug,\n",
    "        lr=training_cfg.lr,\n",
    "        memory_bank_size=training_cfg.memory_bank_size,\n",
    "        temperature=training_cfg.temperature,\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # PEFT / Full Fine-Tuning Logic\n",
    "    # -----------------------------\n",
    "    if training_cfg.use_peft:\n",
    "        print(\"Using PEFT: freezing backbone except last block, training projection head...\")\n",
    "        for name, param in task.backbone.named_parameters():\n",
    "            if \"layer4\" in name:      # optionally fine-tune last residual block\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    else:\n",
    "        print(\"Full fine-tuning: backbone and projection head trainable...\")\n",
    "        for param in task.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Momentum backbone always frozen\n",
    "    for param in task.backbone_momentum.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Projection head always trainable\n",
    "    for param in task.projection_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Example usage for your task\n",
    "    summary_trainable(task)\n",
    "\n",
    "    from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "      dirpath=training_cfg.experiment_out_dir,\n",
    "      filename=\"ssl-best-{epoch:02d}\",\n",
    "      monitor=\"train_loss\",\n",
    "      mode=\"min\",\n",
    "      save_top_k=1,\n",
    "      save_last=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=training_cfg.max_epochs,\n",
    "        enable_progress_bar=True, \n",
    "        log_every_n_steps=num_batches,\n",
    "        precision=16,\n",
    "        accelerator=\"gpu\", # if torch.cuda.is_available() else \"cpu\",\n",
    "        #devices = [0], # training_cfg.devices,\n",
    "\tdeterministic=True,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        logger=logger)\n",
    "    \n",
    "    print(\"USING DEVICE CONFIRMATION\", next(task.parameters()).device)\n",
    "    start_time=time.time()\n",
    "    trainer.fit(task, data_loader, ckpt_path=training_cfg.ckpt_path)\n",
    "    end_time=time.time()\n",
    "    print(\"After fit device:\", next(task.parameters()).device)\n",
    "    print(f\"Training time: {(end_time-start_time)/60} min\")\n",
    "\n",
    "    torch.save(task.backbone.state_dict(),f\"{training_cfg.experiment_out_dir}/ssl_backbone_{timestamp}.pth\")\n",
    "    torch.save(task.projection_head.state_dict(), f\"{training_cfg.experiment_out_dir}/projection_head_{timestamp}.pth\")\n",
    "    trainer.save_checkpoint(f\"{training_cfg.experiment_out_dir}/ssl_ckpt_{timestamp}.ckpt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device =  \"cpu\"\n",
    "    target_num_workers = int(os.cpu_count()*0.75)  # Use 75% of available CPU cores\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"CPU cores available:\", os.cpu_count())\n",
    "    print(\"CPU cores using:\", target_num_workers)\n",
    "\n",
    "    # target_num_workers = 0 # Use 75% of available CPU cores\n",
    "\n",
    "    # target_batch_size = 64   # prefer 256 or 128 depending on GPU memory\n",
    "    # target_patch_size = 264  # Not used in current code but can be added for random cropping\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Calculate dataset statistics\")\n",
    "    parser.add_argument(\n",
    "        \"--data_root_dir\",\n",
    "        type=str,\n",
    "        default=\"/Users/rabinatwayana/1_Rabina/3_CDE_III/ICPR_competition/ICPR-Contest-2026/data/ICPR_SSL_S2A_3k_sample\",\n",
    "        help=\"Path to the root directory of scenes\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_samples\",\n",
    "        type=int,\n",
    "        default=None, # Set to None to use the entire dataset\n",
    "        help=\"Number of samples to calculate statistics on\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\",\n",
    "        type=int,\n",
    "        default=target_num_workers,\n",
    "        help=\"Number of workers for data loading\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    # Training configuration\n",
    "    \n",
    "    data_cfg = DataConfig(\n",
    "        data_root_dir=args.data_root_dir,\n",
    "        compute_stats=False,\n",
    "        n_samples=args.n_samples, # only used for stats calculation, not training\n",
    "        num_workers=args.num_workers,\n",
    "        batch_size=64, # only used for stats calculation, not training\n",
    "        patch_size=264, # only used for stats calculation, not training\n",
    "    )\n",
    "\n",
    "    training_cfg = TrainingConfig(\n",
    "        experiment_out_dir=f\"output/ssl_v2_e50_100_b96_mem_16k\",\n",
    "        model=\"resnet50\",\n",
    "        in_channels=13,\n",
    "        version=2,\n",
    "        lr=1e-4,\n",
    "        use_peft=False,\n",
    "        temperature=0.15,\n",
    "        memory_bank_size= 4096, #16000, #4096, #2048\n",
    "        target_size=224,\n",
    "        batch_size=96,\n",
    "        max_epochs=100,\n",
    "        # ckpt_path = \"/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_50_b96_mem_16k/ssl_ckpt_20260215_104921.ckpt\"\n",
    "    )\n",
    "    main(data_cfg, training_cfg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
