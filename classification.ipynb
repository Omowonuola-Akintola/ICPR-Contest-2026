{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":14933142,"sourceId":126118,"sourceType":"competition"},{"databundleVersionId":15593766,"datasetId":9423234,"sourceId":14744436,"sourceType":"datasetVersion"},{"databundleVersionId":15860753,"modelId":599196,"modelInstanceId":586871,"sourceId":768189,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31287,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Beyond Visible Spectrum: AI for Agriculture 2026\nTask 2:  Boosting Automatic Crop Diseases Classification using Sentinel Satellite Data and Self-Supervised Learning (SSL)\n\nDisclaimer:\n- \"INPUT\" denotes the flag that needs to be updated.","metadata":{}},{"cell_type":"code","source":"# ========================\n# Settining to run the code fully or partially\n# ========================\n\n# If want to skip SSL and downstream training and directly do inference using a saved checkpoint, set this to True\n# The checkpoints used here is from the output of the downstream training in this notebook.\ndo_quick_inference_using_saved_ckpt=True \n\n# To run the SSL and downstream model training from scratch\ndo_ssl_pretraining=False\ndo_downstream_task=False\ndo_hyperparameter_tuning=False\ndo_inference=False\n\n","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:23.222745Z","iopub.execute_input":"2026-02-28T02:16:23.223466Z","iopub.status.idle":"2026-02-28T02:16:23.227576Z","shell.execute_reply.started":"2026-02-28T02:16:23.223427Z","shell.execute_reply":"2026-02-28T02:16:23.226661Z"},"trusted":true},"outputs":[],"execution_count":112},{"cell_type":"code","source":"# ========================\n# Installing necessary libraries\n# ========================\n!pip install torchgeo --quiet\n!pip install optuna --quiet\n!pip install optuna-integration[pytorch_lightning] --quiet","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2026-02-28T02:16:23.229348Z","iopub.execute_input":"2026-02-28T02:16:23.229745Z","iopub.status.idle":"2026-02-28T02:16:35.045737Z","shell.execute_reply.started":"2026-02-28T02:16:23.229717Z","shell.execute_reply":"2026-02-28T02:16:35.044734Z"},"trusted":true},"outputs":[],"execution_count":113},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\n# ========================\n# Imports\n# ========================\nimport os\nimport glob\nimport time\nimport random\nimport argparse\nfrom datetime import datetime\nfrom dataclasses import dataclass\nfrom typing import Optional, Sequence\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n\nimport rasterio\nfrom rasterio.enums import Resampling\nfrom rasterio.warp import Resampling as WarpResampling\n\nfrom torchvision import transforms\nimport kornia.augmentation as K\n\nimport lightning.pytorch as pl\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\nfrom lightning.pytorch.loggers import CSVLogger\n\nfrom torchmetrics.classification import MulticlassF1Score\n\nfrom torchgeo.models import resnet50, ResNet50_Weights\nfrom torchgeo.trainers.moco import MoCoTask\n\nfrom prettytable import PrettyTable\n\nimport optuna\nfrom optuna.integration import PyTorchLightningPruningCallback","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:35.047024Z","iopub.execute_input":"2026-02-28T02:16:35.047333Z","iopub.status.idle":"2026-02-28T02:16:35.066215Z","shell.execute_reply.started":"2026-02-28T02:16:35.047301Z","shell.execute_reply":"2026-02-28T02:16:35.065509Z"},"trusted":true},"outputs":[],"execution_count":114},{"cell_type":"code","source":"# ========================\n# Reproducibility settings\n# ========================\nseed = 42\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\npl.seed_everything(seed, workers=True)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ntorch.use_deterministic_algorithms(True)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(seed)\n","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:35.068171Z","iopub.execute_input":"2026-02-28T02:16:35.068444Z","iopub.status.idle":"2026-02-28T02:16:35.079646Z","shell.execute_reply.started":"2026-02-28T02:16:35.068420Z","shell.execute_reply":"2026-02-28T02:16:35.078957Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Seed set to 42\n","output_type":"stream"},{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c4b9bca5b70>"},"metadata":{}}],"execution_count":115},{"cell_type":"markdown","source":"### Dataset subset code for SSL pre-training","metadata":{}},{"cell_type":"code","source":"'''\nSSL pretraining code for creating subset of data with only one timestamp per folder_id. This is used to create a smaller dataset for faster experimentation. \nThe code selects the middle timestamp folder for each folder_id and copies its contents to a new destination folder.\nThis is not necessary to run.\n'''\n\n# import os\n# import shutil\n\n# # Paths\n# source_root = \"/home/krschap/rabina/data/s2a\"\n# destination_root=\"/home/krschap/rabina/data/s2a_subset\"\n\n# # Make destination folder if it doesn't exist\n# os.makedirs(destination_root, exist_ok=True)\n\n# # Iterate over each folder_id\n# for folder_id in os.listdir(source_root):\n#     folder_path = os.path.join(source_root, folder_id)\n#     if not os.path.isdir(folder_path):\n#         continue\n\n#     # Get list of timestamp folders and sort\n#     timestamps = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n#     if not timestamps:\n#         continue\n\n#     timestamps.sort()  # sort alphabetically or numerically\n#     mid_index = len(timestamps) // 2  # middle folder\n#     mid_timestamp = timestamps[mid_index]\n\n#     # Source and destination paths\n#     src_timestamp_path = os.path.join(folder_path, mid_timestamp)\n#     dst_folder_id_path = os.path.join(destination_root, folder_id)\n#     dst_timestamp_path = os.path.join(dst_folder_id_path, mid_timestamp)\n\n#     # Make destination folder\n#     os.makedirs(dst_timestamp_path, exist_ok=True)\n\n#     # Copy all files from selected timestamp folder\n#     for item in os.listdir(src_timestamp_path):\n#         src_item = os.path.join(src_timestamp_path, item)\n#         dst_item = os.path.join(dst_timestamp_path, item)\n#         if os.path.isdir(src_item):\n#             shutil.copytree(src_item, dst_item)\n#         else:\n#             shutil.copy2(src_item, dst_item)\n\n#     print(f\"Copied {mid_timestamp} from {folder_id} to new folder.\")\n\n# print(\"Done!\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:35.080632Z","iopub.execute_input":"2026-02-28T02:16:35.080856Z","iopub.status.idle":"2026-02-28T02:16:35.086494Z","shell.execute_reply.started":"2026-02-28T02:16:35.080833Z","shell.execute_reply":"2026-02-28T02:16:35.085787Z"},"trusted":true},"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"'\\nSSL pretraining code for creating subset of data with only one timestamp per folder_id. This is used to create a smaller dataset for faster experimentation. \\nThe code selects the middle timestamp folder for each folder_id and copies its contents to a new destination folder.\\nThis is not necessary to run.\\n'"},"metadata":{}}],"execution_count":116},{"cell_type":"code","source":"# To check transforms of the pretrained weight\n# # Load the weight metadata\n# weights = ResNet50_Weights.SENTINEL2_ALL_MOCO\n\n# # 1. Check the built-in transforms\n# print(\"Pre-processing Transforms:\", weights.transforms)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SSL Pre-training related Utility Functions","metadata":{}},{"cell_type":"code","source":"# Timestamp for logging / checkpoints\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n@dataclass\nclass DataConfig:\n    data_root_dir: str = \"/home/krschap/rabina/data/s2a\"\n    num_workers: int = 1\n\n@dataclass\nclass TrainingConfig:\n    experiment_out_dir: str = \"ssl_moco_\"\n    model: str = \"resnet50\"\n    in_channels: int = 13\n    version: int = 2\n    lr: float = 1e-4\n    use_peft: bool = False\n    temperature: float = 0.15\n    memory_bank_size: int = 2048\n    target_size: int = 224\n    max_epochs: int = 100\n    batch_size: int =32\n    ckpt_path: Optional[str] = None\n    weight_decay: float = 1e-4\n    moco_momentum: float = 0.995\n\nclass SSLDataset(Dataset):\n    def __init__(self, scenes, bands, transforms=None, patch_size=264):\n        \"\"\"\n        Args:\n            scenes (list): List of scene folder paths.\n            bands (list): List of band names (e.g., [\"B1\",\"B2\"]).\n            patch_size (tuple): Size of random crop (H, W).\n            transforms (callable, optional): Optional transform to apply to patches.\n        \"\"\"\n        self.scenes = scenes\n        self.bands = bands\n        self.patch_size = patch_size\n        self.transforms = transforms\n        # Precompute all timestamp paths to treat each timestamp as a sample\n        self.samples = []\n        for scene_path in scenes:\n            timestamps = sorted([\n                d for d in os.listdir(scene_path)\n                if os.path.isdir(os.path.join(scene_path, d))\n            ])\n            for ts in timestamps:\n                self.samples.append(os.path.join(scene_path, ts))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ts_path = self.samples[idx]\n\n        band_arrays = []\n\n        for b in self.bands:\n            path = os.path.join(ts_path, f\"{b}.tif\")\n            with rasterio.open(path) as src:\n                if src.height == self.patch_size and src.width == self.patch_size:\n                    arr = src.read(1).astype(np.float32)\n                else:\n                    arr = src.read(\n                        1,\n                        out_shape=(self.patch_size, self.patch_size),\n                        resampling=Resampling.bilinear\n                    ).astype(np.float32)\n\n            band_arrays.append(arr)\n\n        # Insert fake B10\n        insert_idx = 10\n        b10_pad = np.zeros((self.patch_size, self.patch_size), dtype=np.float32)\n        band_arrays.insert(insert_idx, b10_pad)\n\n        img = np.stack(band_arrays, axis=0)\n\n        # img_patch = self._random_crop(img)\n\n        patch_tensor = torch.tensor(img, dtype=torch.float32)\n\n        if self.transforms:\n            patch_tensor = self.transforms(patch_tensor)\n\n        return {\"image\": patch_tensor}\n\ndef summary_trainable(model):\n    table = PrettyTable()\n    table.field_names = [\"Module\", \"Type\", \"Trainable Params\", \"Total Params\"]\n\n    for name, module in model.named_children():\n        total_params = sum(p.numel() for p in module.parameters())\n        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n        table.add_row([name, type(module).__name__, f\"{trainable_params:,}\", f\"{total_params:,}\"])\n\n    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n    \n    print(table)\n    print(f\"Total trainable parameters: {total_trainable:,} ({total_trainable / 1e6:.2f} M)\")\n    print(f\"Total parameters: {total_params:,} ({total_params / 1e6:.2f} M)\")\n\n\n# def main(data_root_dir, n_samples,  batch_size, patch_size, num_workers):\ndef run_ssl_pretraining(data_cfg, training_cfg):\n    print(\"Data root directory:\", data_cfg.data_root_dir)\n    print(\"========================\")\n    print(\"Dataset config:\", data_cfg)\n    print(\"========================\")\n    print(\"Training config:\", training_cfg)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    os.makedirs(training_cfg.experiment_out_dir, exist_ok=True)\n    logger = CSVLogger(\"logs\", name=f\"{training_cfg.experiment_out_dir}/metrics_{timestamp}\")\n\n    aug = K.AugmentationSequential(\n        # K.RandomResizedCrop(size=(training_cfg.target_size, training_cfg.target_size), scale=(0.4, 1.0)),\n        K.RandomHorizontalFlip(),\n        K.RandomVerticalFlip(),\n        K.RandomRotation(degrees=90, p=0.5),\n        K.RandomGaussianBlur(kernel_size=(7,7), sigma=(0.1, 1.5), p=0.3),\n        K.RandomBrightness(brightness=(0.85, 1.15), p=0.5),\n        data_keys=['input'],\n    )\n\n    if not os.path.exists(data_cfg.data_root_dir):\n        raise FileNotFoundError(f\"Data root directory does not exist: {data_cfg.data_root_dir}\")\n    scenes = sorted(glob.glob(os.path.join(data_cfg.data_root_dir, \"*/\")))\n    bands = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"B11\",\"B12\"]\n    # ========================\n    # Train MoCo model\n    # ========================\n    transform = transforms.Compose([\n        transforms.Resize((training_cfg.target_size, training_cfg.target_size)),\n        transforms.Lambda(lambda x: torch.clamp(x, 0, 10000) / 10000.0),\n    ])\n        \n    dataset = SSLDataset(scenes, bands, transforms=transform, patch_size=training_cfg.target_size)\n    print(len(dataset))\n    print(dataset[0]['image'].shape)\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=training_cfg.batch_size,\n        shuffle=True,\n        num_workers=data_cfg.num_workers,\n        worker_init_fn=seed_worker,\n        generator=g\n    )\n    num_batches = len(data_loader)\n    print(\"Number of batches:\", num_batches)\n\n    import time\n    task = MoCoTask(\n        model=training_cfg.model,      \n        weights= ResNet50_Weights.SENTINEL2_ALL_MOCO,\n        in_channels=training_cfg.in_channels,       \n        version=training_cfg.version,             # MoCo v2\n        size=training_cfg.target_size,          \n        augmentation1=aug,\n        augmentation2=aug,\n        lr=training_cfg.lr,\n        weight_decay=training_cfg.weight_decay,\n        memory_bank_size=training_cfg.memory_bank_size,\n        temperature=training_cfg.temperature,\n        moco_momentum=training_cfg.moco_momentum\n    )\n\n    # -----------------------------\n    # PEFT / Full Fine-Tuning Logic\n    # -----------------------------\n    if training_cfg.use_peft:\n        print(\"Using PEFT: freezing backbone except last block, training projection head...\")\n        for name, param in task.backbone.named_parameters():\n            if \"layer4\" in name:      # optionally fine-tune last residual block\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n    else:\n        print(\"Full fine-tuning: backbone and projection head trainable...\")\n        for param in task.backbone.parameters():\n            param.requires_grad = True\n\n    # Momentum backbone always frozen\n    for param in task.backbone_momentum.parameters():\n        param.requires_grad = False\n\n    # Projection head always trainable\n    for param in task.projection_head.parameters():\n        param.requires_grad = True\n\n    # Example usage for your task\n    summary_trainable(task)\n\n    checkpoint_callback = ModelCheckpoint(\n      dirpath=training_cfg.experiment_out_dir,\n      filename=\"ssl-best-{epoch:02d}\",\n      monitor=\"train_loss\",\n      mode=\"min\",\n      save_top_k=1,\n      save_last=True\n    )\n\n    trainer = Trainer(\n        max_epochs=training_cfg.max_epochs,\n        enable_progress_bar=True, \n        log_every_n_steps=num_batches,\n        precision=16,\n        accelerator=\"gpu\", # if torch.cuda.is_available() else \"cpu\",\n        #devices = [0], # training_cfg.devices,\n\t    deterministic=True,\n        callbacks=[checkpoint_callback],\n        logger=logger)\n    \n    print(\"USING DEVICE CONFIRMATION\", trainer.strategy.root_device)\n    start_time=time.time()\n    trainer.fit(task, data_loader, ckpt_path=training_cfg.ckpt_path)\n    end_time=time.time()\n    # print(\"After fit device:\", next(task.parameters()).device)\n    print(f\"Training time: {(end_time-start_time)/60} min\")\n\n    torch.save(task.backbone.state_dict(),f\"{training_cfg.experiment_out_dir}/ssl_backbone_{timestamp}.pth\")\n    torch.save(task.projection_head.state_dict(), f\"{training_cfg.experiment_out_dir}/projection_head_{timestamp}.pth\")\n    trainer.save_checkpoint(f\"{training_cfg.experiment_out_dir}/ssl_ckpt_{timestamp}.ckpt\")","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:35.088201Z","iopub.execute_input":"2026-02-28T02:16:35.088718Z","iopub.status.idle":"2026-02-28T02:16:35.115006Z","shell.execute_reply.started":"2026-02-28T02:16:35.088691Z","shell.execute_reply":"2026-02-28T02:16:35.114412Z"},"trusted":true},"outputs":[],"execution_count":117},{"cell_type":"markdown","source":"### Downstream related unility function\n","metadata":{}},{"cell_type":"code","source":"# Define the dataset class\nclass S2Dataset(torch.utils.data.Dataset):\n    def __init__(self, root_dir, bands, class_idx=None, labeled=True, transform=None, target_size=264):\n        self.root_dir = root_dir\n        self.bands = bands\n        self.labeled = labeled\n        self.transform = transform\n        self.class_idx = class_idx if class_idx is not None else {}\n        self.samples = []\n        self.target_size = target_size\n\n        # all samples\n        if labeled:\n            for cls_name, label in self.class_idx.items():\n                cls_dir = os.path.join(root_dir, cls_name)\n                if not os.path.exists(cls_dir):\n                    continue\n                for img_id in sorted(os.listdir(cls_dir)):\n                    img_dir = os.path.join(cls_dir, img_id)\n                    if os.path.isdir(img_dir):\n                        self.samples.append((img_dir, label))\n        else:\n            for img_id in sorted(os.listdir(root_dir)):\n                img_dir = os.path.join(root_dir, img_id)\n                if os.path.isdir(img_dir):\n                    self.samples.append((img_dir, None))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_dir, label = self.samples[idx]\n        bands_data = []\n        H = W = self.target_size\n        for band in self.bands:\n            band_path = os.path.join(img_dir, f\"{band}.tif\")\n            if os.path.exists(band_path):\n                with rasterio.open(band_path) as src:\n                    arr = src.read(1, out_shape=(H, W), \n                                   resampling=Resampling.bilinear).astype(np.float32)\n            else:\n                arr = np.zeros((H, W), dtype=np.float32)\n            bands_data.append(torch.from_numpy(arr))\n\n        # B10 as zeros\n        if \"B10\" not in self.bands:\n            bands_data.insert(10, torch.zeros((H, W), dtype=torch.float32))\n\n        img = torch.stack(bands_data, dim=0)\n        if self.transform:\n            img = self.transform(img)\n        if self.labeled:\n            return img, label\n        else:\n            img_id = os.path.basename(img_dir)  \n            return img, img_id\n        \n# # MEAN AND STD FROM DATA\n# def calculate_stats(dataset):\n#     # Stack all images into a single tensor of shape (N, 13, H, W)\n#     all_images = torch.stack([img for img, _ in dataset])\n#     # Compute mean and std per band (across N, H, W)\n#     mean = all_images.mean(dim=(0, 2, 3)) # Shape: (13,)\n#     std = all_images.std(dim=(0, 2, 3)) # Shape: (13,)\n#     return mean, std\n\n# SPLIT DATA\ndef stratified_split(dataset, val_ratio=0.2, seed=42):\n    random.seed(seed)\n    class_indices = defaultdict(list)\n    for i, (_, label) in enumerate(dataset.samples):\n        class_indices[label].append(i)\n    train_idx, val_idx = [], []\n    for label, indices in class_indices.items():\n        random.shuffle(indices)\n        n_val = max(1, int(len(indices) * val_ratio))\n        val_idx.extend(indices[:n_val])\n        train_idx.extend(indices[n_val:])\n    return Subset(dataset, train_idx), Subset(dataset, val_idx)\n\n# Weighted sampler + loss weights \ndef weighted_sampler(dataset_subset, num_classes):\n    \"\"\"\n    Creates WeightedRandomSampler based on class frequencies in the subset.    \n    Args:\n        dataset_subset: Subset or Dataset with .samples attribute\n        num_classes: int \n    Returns:\n        WeightedRandomSampler ready for DataLoader\n    \"\"\"\n    # Get labels from the subset\n    labels = [dataset_subset.dataset.samples[i][1] for i in dataset_subset.indices]\n    labels_tensor = torch.tensor(labels)   \n    # Count occurrences per class\n    class_counts = torch.bincount(labels_tensor, minlength=num_classes).float()\n    print(\"Class counts in train split:\", class_counts.tolist()) \n    # Inverse frequency weights\n    weights = 1.0 / (class_counts + 1e-8)           # avoid division by zero\n    weights = weights / weights.sum()                # normalize\n    # Assign weight to each sample\n    sample_weights = [weights[label].item() for label in labels]\n    g = torch.Generator()\n    g.manual_seed(seed)\n    sampler = WeightedRandomSampler(\n        weights=sample_weights,\n        num_samples=len(sample_weights),\n        replacement=True,\n        generator = g\n    )\n    return sampler\n\n\n# Function to load MoCo encoder\ndef load_moco_encoder(\n    checkpoint_path=None,\n    architecture=\"resnet50\",\n    verbose=True\n):\n    if architecture != \"resnet50\":\n        raise NotImplementedError(\"Only resnet50 supported\")\n\n    # torchgeo weights\n    if checkpoint_path is None:\n        if verbose:\n            print(\"Loading TorchGeo Sentinel-2 MoCo weights\")\n        backbone = resnet50(weights=ResNet50_Weights.SENTINEL2_ALL_MOCO)\n\n    # Custom checkpoint\n    else:\n        if verbose:\n            print(f\"Loading custom MoCo checkpoint: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n\n        # Some checkpoints store inside \"state_dict\"\n        if \"state_dict\" in checkpoint:\n            checkpoint = checkpoint[\"state_dict\"]\n\n        # Clean prefixes\n        cleaned = {}\n        for k, v in checkpoint.items():\n            new_key = k\n            for prefix in [\"module.\",\"encoder_q.\", \"encoder.\",\"momentum_encoder.\",\"backbone.\",]:\n                if new_key.startswith(prefix):\n                    new_key = new_key[len(prefix):]\n                    break\n            cleaned[new_key] = v\n\n        # Auto-detect input channel (resnet architecture default is 3)\n        if \"conv1.weight\" not in cleaned:\n            raise ValueError(\"conv1.weight not found in checkpoint\")\n        in_channels = cleaned[\"conv1.weight\"].shape[1]\n        if verbose:\n            print(f\"Detected input channels: {in_channels}\")\n\n        # architecture to match input checkpoint\n        backbone = resnet50(weights=None)\n        backbone.conv1 = nn.Conv2d(\n            in_channels,\n            64, #out_channels\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False,\n        )\n\n        # load weights\n        missing, unexpected = backbone.load_state_dict(cleaned, strict=False)\n        if verbose:\n            print(f\"Missing keys: {len(missing)}\")\n            print(f\"Unexpected keys: {len(unexpected)}\")\n\n    # remove classification head\n    if hasattr(backbone, \"fc\"):\n        backbone.fc = nn.Identity()\n    elif hasattr(backbone, \"head\"):\n        backbone.head = nn.Identity()\n\n    return backbone\n\n\n# Downstream Classifier\nclass MoCoClassifier(pl.LightningModule):\n    def __init__(\n        self,\n        encoder=None,                      # optional: pre-loaded encoder\n        custom_checkpoint_path=None,       # alternative: load from path\n        num_classes=4,\n        lr=3e-3,\n        freeze_backbone=True,\n        focal_gamma=2.0,\n        focal_alpha=None,\n        label_smoothing=0.1,\n        weight_decay=1e-5,\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"encoder\", \"focal_alpha\"])\n\n        # Load or use encoder\n        if encoder is not None:\n            self.encoder = encoder\n            print(\"Using provided encoder\")\n        elif custom_checkpoint_path is not None:\n            self.encoder = load_moco_encoder(checkpoint_path=custom_checkpoint_path)\n        else:\n            self.encoder = load_moco_encoder()  # defaults to TorchGeo\n\n        # Feature dimension\n        in_features = 2048\n        self.classifier = nn.Linear(in_features, num_classes)\n\n        # Freeze backbone for linear probe\n        if freeze_backbone:\n            for p in self.encoder.parameters():\n                p.requires_grad = False\n\n        # Focal loss parameters\n        self.focal_gamma = focal_gamma\n        self.focal_alpha = focal_alpha\n        self.label_smoothing = label_smoothing\n\n        #train \n        self.train_f1_macro = MulticlassF1Score(num_classes=num_classes, average=\"macro\")\n        \n        # Validation metrics\n        self.val_f1_macro = MulticlassF1Score(num_classes=num_classes, average=\"macro\")\n        self.val_f1_weighted = MulticlassF1Score(num_classes=num_classes, average=\"weighted\")\n\n    def forward(self, x):\n        features = self.encoder(x)\n        return self.classifier(features)\n\n    def focal_loss(self, logits, targets):\n        ce_loss = F.cross_entropy(\n            logits,\n            targets,\n            reduction=\"none\",\n            label_smoothing=self.label_smoothing\n        )\n        pt = torch.exp(-ce_loss)\n        focal_weight = (1 - pt) ** self.focal_gamma\n        loss = focal_weight * ce_loss\n\n        if self.focal_alpha is not None:\n            alpha = self.focal_alpha.to(logits.device)\n            alpha_t = alpha[targets]\n            loss = alpha_t * loss\n\n        return loss.mean()\n\n    def _shared_step(self, batch, stage=\"train\"):\n        imgs, labels = batch\n        logits = self(imgs)\n        loss = self.focal_loss(logits, labels)\n    \n        preds = logits.argmax(dim=1)\n        acc = (preds == labels).float().mean()\n    \n        # Log loss and accuracy\n        self.log(f\"{stage}_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(f\"{stage}_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n    \n        # Update and log F1 scores\n        if stage == \"train\":\n            self.train_f1_macro.update(preds, labels)\n            self.log(\"train_f1_macro\", self.train_f1_macro.compute(), on_step=False, on_epoch=True, prog_bar=True)\n            # self.train_f1_macro.reset()\n        elif stage == \"val\":\n            self.val_f1_macro.update(preds, labels)\n            self.val_f1_weighted.update(preds, labels)\n            self.log(\"val_f1_macro\", self.val_f1_macro.compute(), on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"val_f1_weighted\", self.val_f1_weighted.compute(), on_step=False, on_epoch=True, prog_bar=True)\n    \n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self._shared_step(batch, \"train\")\n\n    def validation_step(self, batch, batch_idx):\n        self._shared_step(batch, \"val\")\n\n    def on_train_epoch_end(self):\n        # Log train F1 scores\n        self.log(\"train_f1_macro\", self.train_f1_macro.compute(), prog_bar=True)\n        self.train_f1_macro.reset()\n    \n    def on_validation_epoch_end(self):\n        # Log validation F1 scores\n        self.log(\"val_f1_macro\", self.val_f1_macro.compute(), prog_bar=True)\n        self.log(\"val_f1_weighted\", self.val_f1_weighted.compute(), prog_bar=True)\n        self.val_f1_macro.reset()\n        self.val_f1_weighted.reset()\n\n    def configure_optimizers(self):\n        params = [p for p in self.parameters() if p.requires_grad]\n        optimizer = AdamW(params, lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n\n        scheduler = ReduceLROnPlateau(\n            optimizer,\n            mode=\"max\",\n            factor=0.5,\n            patience=5\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_f1_macro\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1\n            }\n        }\n    \n\n# Training function\ndef train_model(\n    model: pl.LightningModule,\n    train_loader,\n    val_loader,\n    max_epochs: int = 60,\n    monitor_metric: str = \"val_f1_macro\",\n    mode: str = \"max\",\n    patience: int = 10,\n    checkpoint_dir: str = \"/kaggle/working/checkpoints/moco_default\"\n):\n\n    checkpoint_cb = ModelCheckpoint(\n        dirpath=checkpoint_dir,\n        filename=\"moco-{epoch:03d}-{val_f1_macro:.4f}\",\n        monitor=monitor_metric,\n        mode=mode,\n        save_top_k=1,\n        save_last=True,\n        verbose=False\n    )\n\n    early_stop_cb = EarlyStopping(\n        monitor=monitor_metric,\n        mode=mode,\n        patience=patience,\n        min_delta=0.005,\n        verbose=True\n    )\n\n    lr_monitor_cb = LearningRateMonitor(logging_interval=\"epoch\")\n\n    trainer = pl.Trainer(\n        max_epochs=max_epochs,\n        logger=CSVLogger(save_dir=\"/kaggle/working/logs/\", \n                         name=f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\", \n                         version=None),\n        accelerator=\"auto\",\n        deterministic= True,\n        devices=1,\n        precision=\"32\",\n        callbacks=[checkpoint_cb, early_stop_cb, lr_monitor_cb],\n        default_root_dir=checkpoint_dir,\n        log_every_n_steps=10,\n        enable_progress_bar=True\n    )\n\n    start_time = time.time()\n    trainer.fit(model, train_loader, val_loader)\n    end_time = time.time()\n    print(f\"\\nTotal training time: {(end_time-start_time)/60:.2f} minutes\")\n\n    return trainer\n\n# OPTUNA OBJECTIVE \ndef hyperparam_objective(trial, train_dataset, val_dataset, num_classes, alpha): \n    # Define hyperparameters to test\n    hyperparams = {\n        'lr': trial.suggest_float('lr', 1e-5, 1e-2, log=True),\n        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 24]),\n        'focal_gamma': trial.suggest_float('focal_gamma', 1.0, 4.0),\n        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n        'label_smoothing': trial.suggest_float('label_smoothing', 0.0, 0.2),\n    } \n    print(f\"Trial {trial.number}: {hyperparams}\")\n    \n    # Create model with trial hyperparameters\n    model2 = MoCoClassifier(\n        custom_checkpoint_path=custom_checkpoint_path,\n        num_classes=num_classes,\n        lr=hyperparams['lr'],\n        freeze_backbone=True,\n        focal_gamma=hyperparams['focal_gamma'],\n        focal_alpha=alpha,\n        label_smoothing=hyperparams['label_smoothing'],\n        weight_decay=hyperparams['weight_decay'],\n    )\n    \n    # Create data loaders with trial batch size\n    train_loader2 = DataLoader(\n        train_dataset, \n        batch_size=hyperparams['batch_size'], \n        sampler=train_sampler,\n        num_workers=4,\n        pin_memory=True,\n        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n    )\n    val_loader2 = DataLoader(\n        val_dataset,\n        batch_size=hyperparams['batch_size'],\n        shuffle=False,\n        num_workers=4,\n        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id),\n        pin_memory=True\n    )\n    \n    # Using shorter epochs for search and adding pruning callback to stop bad trials early\n    pruning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_f1_macro\")\n    \n    # Create a modified trainer setup for Optuna\n    # checkpoint_cb = ModelCheckpoint(\n    #     monitor=\"val_f1_macro\",\n    #     mode=\"max\",\n    #     save_top_k=1,\n    #     filename=f\"trial_{trial.number}_best\"\n    # )\n    \n    early_stop_cb = EarlyStopping(\n        monitor=\"val_f1_macro\",\n        mode=\"max\",\n        patience=8,  # Shorter patience for quick trials\n        min_delta=0.005\n    )\n    \n    # trainer with pruning\n    trainer2 = pl.Trainer(\n        max_epochs=20,  \n        deterministic=True,\n        accelerator=\"auto\",\n        devices=1,\n        precision=\"32\",\n        callbacks=[early_stop_cb, pruning_callback],\n        enable_progress_bar=True,\n        logger=False,\n    )\n    \n    # Train and return metric\n    trainer2.fit(model2, train_loader2, val_loader2)\n    \n    return trainer2.callback_metrics[\"val_f1_macro\"].item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T02:16:35.115922Z","iopub.execute_input":"2026-02-28T02:16:35.116161Z","iopub.status.idle":"2026-02-28T02:16:35.153454Z","shell.execute_reply.started":"2026-02-28T02:16:35.116138Z","shell.execute_reply":"2026-02-28T02:16:35.152686Z"}},"outputs":[],"execution_count":118},{"cell_type":"markdown","source":"### Defining inputs paths","metadata":{}},{"cell_type":"code","source":"# INPUTS: input_path, evaluation_data_dir, custom_checkpoint_path, output_dir\n\n# dir path for dataset root for downstreaming task\ninput_path = '/kaggle/input/competitions/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle'\n\n# dir path for evaluation data \nevaluation_data_dir = '/kaggle/input/competitions/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle/evaluation'\n\n# checkpoint path obtained from SSL pretraining run\ncustom_checkpoint_path='/kaggle/input/models/rabinatwayana/ssl-subset-v3/pytorch/default/1/ssl_backbone_20260227_074423.pth'\n\n# Output directory for saving the model and logs\noutput_dir = \"/kaggle/working/downstream_sslsubsetv3_v2\"\nos.makedirs(output_dir, exist_ok=True)\n\n# bands in order\nbands = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B11\", \"B12\"] #missing B10\n# classes\nclass_idx = {\"Aphid\": 0, \"Blast\": 1, \"RPH\": 2, \"Rust\": 3}\n\n# load dataset\ndataset = S2Dataset(root_dir=input_path, bands=bands, class_idx=class_idx, labeled=True, transform=None)\n# sanity check\nprint(dataset.__len__())\nimg, label = dataset.__getitem__(0)\nprint(f\"Shape: {img.shape}, Label: {label}\")\n\n# define transform\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Lambda(lambda x: torch.clamp(x, 0, 10000) / 10000.0),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T02:16:35.154316Z","iopub.execute_input":"2026-02-28T02:16:35.154514Z","iopub.status.idle":"2026-02-28T02:16:35.701885Z","shell.execute_reply.started":"2026-02-28T02:16:35.154493Z","shell.execute_reply":"2026-02-28T02:16:35.701014Z"}},"outputs":[{"name":"stdout","text":"900\nShape: torch.Size([13, 264, 264]), Label: 0\n","output_type":"stream"}],"execution_count":119},{"cell_type":"markdown","source":"# ONLY RUNNING INFERENCE USING SAVED CHECKPOINTS","metadata":{}},{"cell_type":"code","source":"if do_quick_inference_using_saved_ckpt:\n\n    # INPUT: downstream_ckpt_path\n\n    # Provide the path to the downstream trained checkpoint you want to use for inference. \n    downstream_ckpt_path= \"/kaggle/input/models/rabinatwayana/downstream-ckpt-v2/pytorch/sslv3/1/moco-epoch059-val_f1_macro0.6872.ckpt\"\n    \n    # prediction\n    test_dataset = S2Dataset(root_dir=evaluation_data_dir, bands=bands, class_idx=class_idx, labeled=False, transform=transform)\n    # sanity check\n    print(test_dataset.__len__())\n    img, img_id = test_dataset.__getitem__(0)\n    print(f\"Shape: {img.shape}\")\n\n    # INPUT: checkpoint_path\n    \n    # Load the downstream trained best checkpoint for inference\n    model = MoCoClassifier.load_from_checkpoint(downstream_ckpt_path)\n    model.eval()\n    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Define class names using your class_idx\n    class_names = {v: k for k, v in class_idx.items()}  # Reverse class_idx to map indices to names\n    \n    # Create the test DataLoader\n    test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False,num_workers=4,\n                            worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id))\n\n    # predictions\n    image_ids = []\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            imgs, img_ids = batch  # img_ids are the folder names\n            imgs = imgs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            # Get model predictions\n            logits = model(imgs)\n            preds = logits.argmax(dim=1).cpu().numpy()\n            image_ids.extend(img_ids)\n            predictions.extend(preds)\n    \n    # Map predictions to class names\n    predicted_classes = [class_names[pred] for pred in predictions]\n    \n    # Save to CSV\n    results_df = pd.DataFrame({\n        \"Id\": image_ids,\n        \"Category\": predicted_classes\n    })\n    \n    results_df.to_csv(\"submission.csv\", index=False)\n    results_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T02:16:35.704389Z","iopub.execute_input":"2026-02-28T02:16:35.704684Z","iopub.status.idle":"2026-02-28T02:16:39.339640Z","shell.execute_reply.started":"2026-02-28T02:16:35.704656Z","shell.execute_reply":"2026-02-28T02:16:39.338733Z"}},"outputs":[{"name":"stdout","text":"40\nShape: torch.Size([13, 224, 224])\nLoading custom MoCo checkpoint: /kaggle/input/models/rabinatwayana/ssl-subset-v3/pytorch/default/1/ssl_backbone_20260227_074423.pth\nDetected input channels: 13\nMissing keys: 2\nUnexpected keys: 0\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"if do_quick_inference_using_saved_ckpt:\n    raise SystemExit(\"Stopping notebook execution here intentionally to prevent from deleting cell output below\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T02:16:39.340891Z","iopub.execute_input":"2026-02-28T02:16:39.341267Z","iopub.status.idle":"2026-02-28T02:16:39.346725Z","shell.execute_reply.started":"2026-02-28T02:16:39.341225Z","shell.execute_reply":"2026-02-28T02:16:39.345789Z"}},"outputs":[{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Stopping notebook execution here intentionally to prevent from deleting cell output below\n"],"ename":"SystemExit","evalue":"Stopping notebook execution here intentionally to prevent from deleting cell output below","output_type":"error"}],"execution_count":121},{"cell_type":"markdown","source":"### Running ssl-pretraining","metadata":{}},{"cell_type":"code","source":"# INPUTS: data_root_dir, experiment_out_dir\n'''\nThis part was run in personal computer, the log is added in cell below. But can be run here aswell.\n'''\n# if __name__ == \"__main__\":\nif do_ssl_pretraining:\n    data_root_dir=\"/kaggle/input/datasets/rabinatwayana/icpr-2026-competition-ssl-s2a-3k-subset/ICPR_SSL_S2A_3k_sample\"\n    experiment_out_dir= \"/kaggle/working/ssl_subset_v3_e50\"\n    target_num_workers = int(os.cpu_count()*0.75)  # *0.75 Use 75% of available CPU cores\n    print(\"CPU cores available:\", os.cpu_count())\n    print(\"CPU cores using:\", target_num_workers)\n\n    # Training configuration\n    data_cfg = DataConfig(\n        data_root_dir=data_root_dir,\n        num_workers=target_num_workers,\n    )\n\n    training_cfg = TrainingConfig(\n        experiment_out_dir=experiment_out_dir,\n        model=\"resnet50\",\n        in_channels=13,\n        version=2,\n        lr=2e-4,\n        use_peft=False,\n        temperature=0.15,\n        memory_bank_size= 4096, #4096, #16000, #4096, #2048\n        target_size=224,\n        batch_size=128, #128, #256, #64, #32\n        weight_decay=1e-4,\n        moco_momentum=0.995,\n        max_epochs=50,\n        # schedule=[60, 80],\n        # ckpt_path =x \"/home/krschap/rabina/ICPR-Contest-2026/output/ssl_v1_e20_50_b96_mem_16k/ssl_ckpt_20260215_104921.ckpt\" # Path to checkpoint for resuming training (optional\n    )\n    \n    run_ssl_pretraining(data_cfg, training_cfg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T02:16:39.347366Z","iopub.status.idle":"2026-02-28T02:16:39.347676Z","shell.execute_reply.started":"2026-02-28T02:16:39.347536Z","shell.execute_reply":"2026-02-28T02:16:39.347556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Logs for the SSL-pretraining as it was run in CPU not in kaggle (for reference, not necessary to run):\n# -----------------------------\n# Seed set to 42\n# CPU cores available: 32\n# CPU cores using: 24\n# Data root directory: /home/krschap/rabina/data/s2a_subset\n# ========================\n# Dataset config: DataConfig(data_root_dir='/home/krschap/rabina/data/s2a_subset', num_workers=24)\n# ========================\n# Training config: TrainingConfig(experiment_out_dir='output/ssl_subset_v3_e50_b256_mem_8k_rm_norm', model='resnet50', in_channels=13, version=2, lr=0.0002, use_peft=False, temperature=0.15, memory_bank_size=4096, target_size=224, max_epochs=50, batch_size=128, ckpt_path=None, weight_decay=0.0001, moco_momentum=0.995)\n# 32567\n# torch.Size([13, 224, 224])\n# Number of batches: 255\n# /home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/torchgeo/trainers/moco.py:209: UserWarning: MoCo v2 only uses 2 layers in its projection head\n#   warnings.warn('MoCo v2 only uses 2 layers in its projection head')\n# Full fine-tuning: backbone and projection head trainable...\n# +--------------------------+------------------------+------------------+--------------+\n# |          Module          |          Type          | Trainable Params | Total Params |\n# +--------------------------+------------------------+------------------+--------------+\n# |         backbone         |         ResNet         |    23,539,392    |  23,539,392  |\n# |    backbone_momentum     |         ResNet         |        0         |  23,539,392  |\n# |     projection_head      |   MoCoProjectionHead   |    26,222,848    |  26,222,848  |\n# | projection_head_momentum |   MoCoProjectionHead   |        0         |  26,222,848  |\n# |        criterion         |       NTXentLoss       |        0         |      0       |\n# |      augmentation1       | AugmentationSequential |        0         |      0       |\n# +--------------------------+------------------------+------------------+--------------+\n# Total trainable parameters: 49,762,240 (49.76 M)\n# Total parameters: 99,524,480 (99.52 M)\n# /home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n# Using 16bit Automatic Mixed Precision (AMP)\n# GPU available: True (cuda), used: True\n# TPU available: False, using: 0 TPU cores\n# üí° Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n# USING DEVICE CONFIRMATION cuda:0\n# /home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n# You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n# LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n# /home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n# ‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n# ‚îÉ   ‚îÉ Name                     ‚îÉ Type                   ‚îÉ Params ‚îÉ Mode  ‚îÉ FLOPs ‚îÉ\n# ‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n# ‚îÇ 0 ‚îÇ backbone                 ‚îÇ ResNet                 ‚îÇ 23.5 M ‚îÇ train ‚îÇ     0 ‚îÇ\n# ‚îÇ 1 ‚îÇ backbone_momentum        ‚îÇ ResNet                 ‚îÇ 23.5 M ‚îÇ train ‚îÇ     0 ‚îÇ\n# ‚îÇ 2 ‚îÇ projection_head          ‚îÇ MoCoProjectionHead     ‚îÇ 26.2 M ‚îÇ train ‚îÇ     0 ‚îÇ\n# ‚îÇ 3 ‚îÇ projection_head_momentum ‚îÇ MoCoProjectionHead     ‚îÇ 26.2 M ‚îÇ train ‚îÇ     0 ‚îÇ\n# ‚îÇ 4 ‚îÇ criterion                ‚îÇ NTXentLoss             ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n# ‚îÇ 5 ‚îÇ augmentation1            ‚îÇ AugmentationSequential ‚îÇ      0 ‚îÇ train ‚îÇ     0 ‚îÇ\n# ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n# Trainable params: 49.8 M\n# Non-trainable params: 49.8 M\n# Total params: 99.5 M\n# Total estimated model params size (MB): 398\n# Modules in train mode: 460\n# Modules in eval mode: 0\n# Total FLOPs: 0\n# /home/krschap/rabina/ICPR-Contest-2026/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n# Epoch 49/49 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 255/255 0:01:35 ‚Ä¢ 0:00:00 2.81it/s v_num: 0.000`Trainer.fit` stopped: `max_epochs=50` reached.\n# Epoch 49/49 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 255/255 0:01:35 ‚Ä¢ 0:00:00 2.81it/s v_num: 0.000\n# Training time: 79.93534090121587 min\n# `weights_only` was not set, defaulting to `False`.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T02:16:39.349862Z","iopub.status.idle":"2026-02-28T02:16:39.350155Z","shell.execute_reply.started":"2026-02-28T02:16:39.350016Z","shell.execute_reply":"2026-02-28T02:16:39.350037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Downstream Task","metadata":{}},{"cell_type":"code","source":"# apply transform\ndataset = S2Dataset(root_dir=input_path, bands=bands, class_idx=class_idx, labeled=True, transform=transform)\n\n# split data\ntrain_dataset, val_dataset = stratified_split(dataset, val_ratio=0.2)\n# Create sampler for train\nnum_classes = len(class_idx)\ntrain_sampler = weighted_sampler(train_dataset, num_classes=num_classes)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, sampler=train_sampler,\n                          num_workers=4,pin_memory=True,\n                         worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id))\nval_loader = DataLoader(val_dataset,batch_size=16,shuffle=False,num_workers=4,pin_memory=True,\n                       worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id))\n\n# Loss weights (for Focal loss) \nlabels = torch.tensor([label for _, label in dataset.samples]) #collect all labels\nclass_counts = torch.bincount(labels, minlength=num_classes).float() #num of images per class\nclass_counts_for_loss = class_counts  \nalpha = 1.0 / (class_counts_for_loss + 1e-8) #compute raw inverse frequency, used 1e-8 to avoid division by 0\nalpha = alpha / alpha.sum() * num_classes   # normalize so average ~1\nprint(\"Loss/Focal alpha weights:\", alpha.tolist())","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:39.351036Z","iopub.status.idle":"2026-02-28T02:16:39.351352Z","shell.execute_reply.started":"2026-02-28T02:16:39.351169Z","shell.execute_reply":"2026-02-28T02:16:39.351215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Running Hyperparameter tuning - optuna","metadata":{}},{"cell_type":"code","source":"# RUN OPTUNA STUDY\n# Create study\nif do_hyperparameter_tuning:    \n    study = optuna.create_study(\n        direction=\"maximize\",\n        study_name=\"moco_hyperparams\",\n        pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n        sampler = optuna.samplers.TPESampler(seed= seed)\n    )\n    import time\n    start_time=time.time()\n    # Run trials \n    study.optimize(\n        lambda trial: hyperparam_objective(trial, train_dataset, val_dataset, num_classes, alpha),\n        n_trials=10,\n        show_progress_bar=True\n    )\n    end_time=time.time()\n    print(f\"Training time: {(end_time-start_time)/60} min\")\n    \n    # Show results\n    print(\"Best hyperparameters:\")\n    print(f\"Best F1 Score: {study.best_value:.4f}\")\n    print(\"\\nOptimal parameters:\")\n    for key, value in study.best_params.items():\n        print(f\"  {key}: {value}\")\n    # SAVE RESULTS\n    import json\n    results = {\n        \"best_f1_score\": float(study.best_value),\n        \"best_params\": study.best_params,\n        \"num_trials\": len(study.trials),\n    }\n    \n    with open(f\"{output_dir}/best_hyperparams.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-28T02:16:39.353138Z","iopub.status.idle":"2026-02-28T02:16:39.353813Z","shell.execute_reply.started":"2026-02-28T02:16:39.353576Z","shell.execute_reply":"2026-02-28T02:16:39.353611Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train model with best hyper-parameters and SSL pretrained backbone(weights)","metadata":{}},{"cell_type":"code","source":"if do_hyperparameter_tuning:\n    import json\n    with open(f\"{output_dir}/best_hyperparams.json\", \"r\") as f:\n        saved_results = json.load(f)\n    \n    best_params = saved_results[\"best_params\"]\n    print(best_params)\nelse:\n    # if not json file saved, update following variable based on output of hyperparameter tuning\n    best_params={\n      \"lr\": 0.003967605077052989,\n      \"batch_size\":16,\n      \"focal_gamma\":3.909729556485983,\n      \"weight_decay\":0.0003142880890840109,\n      \"label_smoothing\":0.04246782213565523\n    }","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:39.354904Z","iopub.status.idle":"2026-02-28T02:16:39.355362Z","shell.execute_reply.started":"2026-02-28T02:16:39.355112Z","shell.execute_reply":"2026-02-28T02:16:39.355143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if do_downstream_task:\n    # Create model with best params\n    final_model = MoCoClassifier(\n        custom_checkpoint_path=custom_checkpoint_path,\n        num_classes=num_classes,\n        lr=best_params['lr'],\n        freeze_backbone=True,\n        focal_gamma=best_params['focal_gamma'],\n        focal_alpha=alpha,\n        label_smoothing=best_params['label_smoothing'],\n        weight_decay=best_params['weight_decay'],\n    )\n    \n    # Data loaders with best batch size\n    train_sampler = weighted_sampler(train_dataset, num_classes)\n    final_train_loader = DataLoader(\n        train_dataset, \n        batch_size=best_params['batch_size'], \n        sampler=train_sampler,\n        num_workers=4,\n        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n    )\n    final_val_loader = DataLoader(\n        val_dataset,\n        batch_size=best_params['batch_size'],\n        shuffle=False,\n        num_workers=4,\n        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n    )\n\n    import time\n    start_time=time.time()\n    # Train final model\n    final_trainer = train_model(\n        model=final_model,\n        train_loader=final_train_loader,\n        val_loader=final_val_loader,\n        max_epochs=100,\n        monitor_metric=\"val_f1_macro\",\n        mode=\"max\",\n        patience=30,\n        checkpoint_dir=f\"{output_dir}/checkpoints_final_tuned\"\n    )\n    end_time=time.time()\n    print(f\"Training time: {(end_time-start_time)/60} min\")","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:39.357107Z","iopub.status.idle":"2026-02-28T02:16:39.357570Z","shell.execute_reply.started":"2026-02-28T02:16:39.357350Z","shell.execute_reply":"2026-02-28T02:16:39.357370Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# move logs to output dir\n!mv /kaggle/working/logs /kaggle/working/downstream_sslsubsetv3_v2","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:39.358836Z","iopub.status.idle":"2026-02-28T02:16:39.359181Z","shell.execute_reply.started":"2026-02-28T02:16:39.358999Z","shell.execute_reply":"2026-02-28T02:16:39.359042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#INPUT: output_metrics_path\noutput_metrics_path=\"/kaggle/working/downstream_sslsubsetv3_v2/logs/exp_20260228_013641/version_0/metrics.csv\"\n\n# Load the CSV file\nmetrics_df = pd.read_csv(output_metrics_path)\nmetrics_df_ = metrics_df.dropna(how='all') # Drop rows where all values are NaN\nmetrics_df_ = metrics_df_.ffill() # Forward-fill NaN values (carry last valid observation forward)\nmetrics_df_ = metrics_df_.groupby('epoch').last().reset_index()\n\n# plots\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].plot(metrics_df_[\"epoch\"], metrics_df_[\"train_loss\"], label=\"Train Loss\")\naxes[0].plot(metrics_df_[\"epoch\"], metrics_df_[\"val_loss\"], label=\"Validation Loss\")\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].set_title(\"Train vs. Validation Loss\")\naxes[0].legend()\naxes[0].grid(True)\n\naxes[1].plot(metrics_df_[\"epoch\"], metrics_df_[\"train_f1_macro\"], label=\"Train F1 Macro\")\naxes[1].plot(metrics_df_[\"epoch\"], metrics_df_[\"val_f1_macro\"], label=\"Validation F1 Macro\")\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"F1 Macro\")\naxes[1].set_title(\"Train vs. Validation F1 Macro\")\naxes[1].legend()\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:39.360698Z","iopub.status.idle":"2026-02-28T02:16:39.361104Z","shell.execute_reply.started":"2026-02-28T02:16:39.360866Z","shell.execute_reply":"2026-02-28T02:16:39.360900Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# INPUT: downstream_ckpt_path\n# path to the best checkpoint of the downstream task\ndownstream_ckpt_path= \"/kaggle/working/downstream_sslsubsetv3_v2/checkpoints_final_tuned/moco-epoch=059-val_f1_macro=0.6872.ckpt\"\n\nif do_inference:\n    # prediction\n    test_dataset = S2Dataset(root_dir=evaluation_data_dir, bands=bands, class_idx=class_idx, labeled=False, transform=transform)\n    # sanity check\n    print(test_dataset.__len__())\n    img, img_id = test_dataset.__getitem__(0)\n    print(f\"Shape: {img.shape}\")\n\n    # INPUT: checkpoint_path\n    \n    # Load the downstream trained best checkpoint for inference\n    # checkpoint_path = downstream_ckpt_path\n    model = MoCoClassifier.load_from_checkpoint(downstream_ckpt_path)\n    model.eval()\n    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Define class names using your class_idx\n    class_names = {v: k for k, v in class_idx.items()}  # Reverse class_idx to map indices to names\n    \n    # Create the test DataLoader\n    test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False,num_workers=4,\n                            worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id))\n\n    # predictions\n    image_ids = []\n    predictions = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            imgs, img_ids = batch  # img_ids are the folder names\n            imgs = imgs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            # Get model predictions\n            logits = model(imgs)\n            preds = logits.argmax(dim=1).cpu().numpy()\n            image_ids.extend(img_ids)\n            predictions.extend(preds)\n    \n    # Map predictions to class names\n    predicted_classes = [class_names[pred] for pred in predictions]\n    \n    # Save to CSV\n    results_df = pd.DataFrame({\n        \"Id\": image_ids,\n        \"Category\": predicted_classes\n    })\n    \n    results_df.to_csv(f\"{output_dir}/submission.csv\", index=False)\n    results_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:39.362073Z","iopub.status.idle":"2026-02-28T02:16:39.362440Z","shell.execute_reply.started":"2026-02-28T02:16:39.362240Z","shell.execute_reply":"2026-02-28T02:16:39.362267Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zip the output directory for download\nimport shutil\n\nshutil.make_archive(\"/kaggle/working/downstream_sslsubsetv3_v2\", 'zip', \"/kaggle/working/downstream_sslsubsetv3_v2\")","metadata":{"execution":{"iopub.status.busy":"2026-02-28T02:16:39.363688Z","iopub.status.idle":"2026-02-28T02:16:39.364110Z","shell.execute_reply.started":"2026-02-28T02:16:39.363899Z","shell.execute_reply":"2026-02-28T02:16:39.363929Z"},"trusted":true},"outputs":[],"execution_count":null}]}