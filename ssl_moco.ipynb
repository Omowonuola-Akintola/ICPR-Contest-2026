{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.enums import Resampling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from lightning.pytorch import Trainer\n",
    "from torchvision import transforms  \n",
    "from torchgeo.trainers.moco import MoCoTask\n",
    "from torchgeo.models import ResNet50_Weights\n",
    "import kornia.augmentation as K\n",
    "import torch.nn.functional as F\n",
    "import torchgeo.transforms as T\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "random.seed(42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info from metadata.json\n",
    "# B1\t1830 × 1830\t60 m\n",
    "# B2\t10980 × 10980\t10 m\n",
    "# B3\t10980 × 10980\t10 m\n",
    "# B4\t10980 × 10980\t10 m\n",
    "# B5\t5490 × 5490\t20 m\n",
    "# B6\t5490 × 5490\t20 m\n",
    "# B7\t5490 × 5490\t20 m\n",
    "# B8\t10980 × 10980\t10 m\n",
    "# B8A\t5490 × 5490\t20 m\n",
    "# B9\t1830 × 1830\t60 m\n",
    "# B11\t5490 × 5490\t20 m\n",
    "# B12\t5490 × 5490\t20 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ac243",
   "metadata": {},
   "source": [
    "### Sub-sample 3k Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c797af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = \"/Volumes/WD_Rabina/competition/extracted_data/s2a\"\n",
    "# # List all folders\n",
    "# scenes = sorted(glob.glob(os.path.join(root_dir, \"*/\")))\n",
    "# # print(scenes)\n",
    "# print(len(scenes))\n",
    "\n",
    "# no_of_files=3000\n",
    "\n",
    "# # Randomly select 3000 scenes (without replacement)\n",
    "# selected_scenes = random.sample(scenes, k=3000)\n",
    "\n",
    "# print(f\"Total selected scenes: {len(selected_scenes)}\")\n",
    "# # print(selected_scenes[:10])  # show first 10 for sanity check\n",
    "\n",
    "# # Path to new folder where selected scenes will be copied\n",
    "# destination_root = \"data/s2a_3k_sample\"\n",
    "# os.makedirs(destination_root, exist_ok=True)  # create folder if it doesn't exist\n",
    "\n",
    "# # Copy each selected folder\n",
    "# count=0\n",
    "# for scene_path in selected_scenes:\n",
    "#     # Get folder name only (e.g., \"000015\")\n",
    "#     folder_name = os.path.basename(os.path.normpath(scene_path))\n",
    "    \n",
    "#     # Destination path\n",
    "#     dest_path = os.path.join(destination_root, folder_name)\n",
    "#     count=count+1\n",
    "#     print(count)\n",
    "#     # Copy folder and all its contents\n",
    "#     shutil.copytree(scene_path, dest_path)\n",
    "\n",
    "# print(f\"Copied {len(selected_scenes)} folders to {destination_root}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c900bb",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 224\n",
    "target_batch_size=64 #prefer 256 or 128\n",
    "logger = CSVLogger(\"logs\", name=\"moco_run\")\n",
    "target_num_workers=0\n",
    "target_max_epoch=3\n",
    "\n",
    "use_peft = False  \n",
    "\n",
    "# Default\n",
    "# # Similar to SimCLR: https://arxiv.org/abs/2002.05709\n",
    "# aug1 = aug2 = K.AugmentationSequential(\n",
    "#     K.RandomResizedCrop(size=(size, size), scale=(0.2, 1)),\n",
    "#     K.RandomBrightness(brightness=(0.6, 1.4), p=0.8),\n",
    "#     K.RandomContrast(contrast=(0.6, 1.4), p=0.8),\n",
    "#     T.RandomGrayscale(weights=weights, p=0.2),\n",
    "#     K.RandomGaussianBlur(kernel_size=(ks, ks), sigma=(0.1, 2), p=0.5),\n",
    "#     K.RandomHorizontalFlip(),\n",
    "#     K.RandomVerticalFlip(),  # added\n",
    "#     data_keys=['input'],\n",
    "# )\n",
    "\n",
    "aug = K.AugmentationSequential(\n",
    "    K.RandomResizedCrop(size=(target_size, target_size), scale=(0.4, 1.0)),\n",
    "    K.RandomHorizontalFlip(),\n",
    "    K.RandomVerticalFlip(),\n",
    "    K.RandomGaussianBlur(kernel_size=(7,7), sigma=(0.1, 1.5), p=0.3),\n",
    "    K.RandomBrightness(brightness=(0.85, 1.15), p=0.5),\n",
    "    data_keys=['input'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246afb3c",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f66218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, scenes, bands, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scenes (list): List of scene folder paths.\n",
    "            bands (list): List of band names (e.g., [\"B1\",\"B2\"]).\n",
    "            patch_size (tuple): Size of random crop (H, W).\n",
    "            transforms (callable, optional): Optional transform to apply to patches.\n",
    "        \"\"\"\n",
    "        self.scenes = scenes\n",
    "        self.bands = bands\n",
    "        # self.patch_size = patch_size\n",
    "        self.transforms = transforms\n",
    "        self.target_h= None\n",
    "        self.target_w = None\n",
    "        \n",
    "\n",
    "        # Precompute all timestamp paths to treat each timestamp as a sample\n",
    "        self.samples = []\n",
    "        for scene_path in scenes:\n",
    "            timestamps = sorted([\n",
    "                d for d in os.listdir(scene_path)\n",
    "                if os.path.isdir(os.path.join(scene_path, d))\n",
    "            ])\n",
    "            for ts in timestamps:\n",
    "                self.samples.append(os.path.join(scene_path, ts))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ts_path = self.samples[idx]\n",
    "\n",
    "        with rasterio.open(os.path.join(ts_path, \"B2.tif\")) as src:\n",
    "            target_h, target_w = src.height, src.width\n",
    "            # print(target_h,target_w, \"target width and height\" )\n",
    "\n",
    "        band_arrays = []\n",
    "\n",
    "        for b in self.bands:\n",
    "            path = os.path.join(ts_path, f\"{b}.tif\")\n",
    "            with rasterio.open(path) as src:\n",
    "                if src.height == target_h and src.width == target_w:\n",
    "                    arr = src.read(1).astype(np.float32)\n",
    "                else:\n",
    "                    arr = src.read(\n",
    "                        1,\n",
    "                        out_shape=(target_h, target_w),\n",
    "                        resampling=Resampling.bilinear\n",
    "                    ).astype(np.float32)\n",
    "\n",
    "            band_arrays.append(arr)\n",
    "\n",
    "        # Insert fake B10\n",
    "        insert_idx = 10\n",
    "        b10_pad = np.zeros((target_h, target_w), dtype=np.float32)\n",
    "        band_arrays.insert(insert_idx, b10_pad)\n",
    "\n",
    "        img = np.stack(band_arrays, axis=0)\n",
    "\n",
    "        # img_patch = self._random_crop(img)\n",
    "\n",
    "        patch_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "        if self.transforms:\n",
    "            patch_tensor = self.transforms(patch_tensor)\n",
    "\n",
    "        return {\"image\": patch_tensor}\n",
    "\n",
    "def calculate_stats(dataset, n_samples=500):\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    print(len(dataset))\n",
    "    n = min(len(dataset), n_samples)\n",
    "\n",
    "    for i in range(n):\n",
    "        sample = dataset[i]\n",
    "        img = sample[\"image\"]   # <-- key fix\n",
    "\n",
    "        mean += img.mean(dim=(1, 2))\n",
    "        std += img.std(dim=(1, 2))\n",
    "\n",
    "    mean /= n\n",
    "    std /= n\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data/s2a\"\n",
    "# List all folders\n",
    "scenes = sorted(glob.glob(os.path.join(root_dir, \"*/\")))\n",
    "# scenes = [\"data/s2a/000015\", \"data/s2a/000016\"]  # list of scene folders\n",
    "bands = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"B11\",\"B12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6322bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time run to get mean and std\n",
    "temp_dataset = SSLDataset(scenes, bands)\n",
    "mean, std = calculate_stats(temp_dataset)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [2358.7412, 2402.7629, 2580.9255, 2614.2227, 3057.6877, 3578.1008,\n",
    "        3796.8345, 3795.6868, 3947.5913, 4833.6362,    0.0000, 3379.1743,\n",
    "        2666.4465]\n",
    "std = [2994.4861, 2847.0354, 2542.9307, 2411.1196, 2399.0249, 2137.6804,\n",
    "        2036.8357, 2042.7140, 1957.9615, 3559.4121,    0.0000, 1535.7960,\n",
    "        1393.8278]\n",
    "\n",
    "# to avoid 0 std\n",
    "std = [max(s, 1e-5) for s in std]   \n",
    "\n",
    "# define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((target_size, target_size)),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SSLDataset(scenes, bands, transforms=transform)\n",
    "print(len(dataset))\n",
    "print(dataset[0]['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=target_batch_size, shuffle=True, num_workers=target_num_workers)\n",
    "num_batches = len(data_loader)\n",
    "print(\"Number of batches:\", num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7c92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W205 17:00:02.187859000 NNPACK.cpp:56] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "task = MoCoTask(\n",
    "    model=\"resnet50\",      \n",
    "    weights=ResNet50_Weights.SENTINEL2_ALL_MOCO,\n",
    "    in_channels=13,       \n",
    "    version=2,             # MoCo v2\n",
    "    size=target_size,          \n",
    "    augmentation1=aug,\n",
    "    augmentation2=aug,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# PEFT / Full Fine-Tuning Logic\n",
    "# -----------------------------\n",
    "if use_peft:\n",
    "    print(\"Using PEFT: freezing backbone except last block, training projection head...\")\n",
    "    for name, param in task.backbone.named_parameters():\n",
    "        if \"layer4\" in name:      # optionally fine-tune last residual block\n",
    "            # print(\"Layer 4 trainable\")\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "else:\n",
    "    print(\"Full fine-tuning: backbone and projection head trainable...\")\n",
    "    for param in task.backbone.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Momentum backbone always frozen\n",
    "for param in task.backbone_momentum.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Projection head always trainable\n",
    "for param in task.projection_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=target_max_epoch,\n",
    "    enable_progress_bar=True, \n",
    "    log_every_n_steps=1,\n",
    "    precision=16,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    logger=logger)\n",
    "trainer.fit(task, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task.trainer.logged_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0814d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the backbone encoder only\n",
    "torch.save(task.backbone.state_dict(), \"ssl_encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"ssl_full_ckpt.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
