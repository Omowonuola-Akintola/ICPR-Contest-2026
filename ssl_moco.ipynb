{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":126118,"databundleVersionId":14933142,"sourceType":"competition"},{"sourceId":14744436,"sourceType":"datasetVersion","datasetId":9423234}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# full, 50 epoch, new std, mean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.022090Z","iopub.execute_input":"2026-02-06T09:49:34.022626Z","iopub.status.idle":"2026-02-06T09:49:34.025828Z","shell.execute_reply.started":"2026-02-06T09:49:34.022595Z","shell.execute_reply":"2026-02-06T09:49:34.025048Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# !pip install torchgeo --quiet\n# !pip install lightning --quiet\n# !pip install prettytable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.029580Z","iopub.execute_input":"2026-02-06T09:49:34.030119Z","iopub.status.idle":"2026-02-06T09:49:34.038095Z","shell.execute_reply.started":"2026-02-06T09:49:34.030089Z","shell.execute_reply":"2026-02-06T09:49:34.037423Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.039213Z","iopub.execute_input":"2026-02-06T09:49:34.039480Z","iopub.status.idle":"2026-02-06T09:49:34.049202Z","shell.execute_reply.started":"2026-02-06T09:49:34.039457Z","shell.execute_reply":"2026-02-06T09:49:34.048487Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# os.cpu_count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.050080Z","iopub.execute_input":"2026-02-06T09:49:34.050434Z","iopub.status.idle":"2026-02-06T09:49:34.059211Z","shell.execute_reply.started":"2026-02-06T09:49:34.050396Z","shell.execute_reply":"2026-02-06T09:49:34.058524Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset\nimport rasterio\nimport numpy as np\nfrom rasterio.enums import Resampling\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torchvision.models import resnet50\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as T\nfrom lightning.pytorch import Trainer\nfrom torchvision import transforms  \nfrom torchgeo.trainers.moco import MoCoTask\nfrom torchgeo.models import ResNet50_Weights\nimport kornia.augmentation as K\nimport torch.nn.functional as F\nimport torchgeo.transforms as T\nfrom lightning.pytorch.loggers import CSVLogger\nimport glob\nimport os\nimport shutil\nimport random\nimport time\nfrom prettytable import PrettyTable\nrandom.seed(42) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.060362Z","iopub.execute_input":"2026-02-06T09:49:34.060604Z","iopub.status.idle":"2026-02-06T09:49:34.071721Z","shell.execute_reply.started":"2026-02-06T09:49:34.060565Z","shell.execute_reply":"2026-02-06T09:49:34.070975Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Info from metadata.json\n# B1\t1830 Ã— 1830\t60â€¯m\n# B2\t10980 Ã— 10980\t10â€¯m\n# B3\t10980 Ã— 10980\t10â€¯m\n# B4\t10980 Ã— 10980\t10â€¯m\n# B5\t5490 Ã— 5490\t20â€¯m\n# B6\t5490 Ã— 5490\t20â€¯m\n# B7\t5490 Ã— 5490\t20â€¯m\n# B8\t10980 Ã— 10980\t10â€¯m\n# B8A\t5490 Ã— 5490\t20â€¯m\n# B9\t1830 Ã— 1830\t60â€¯m\n# B11\t5490 Ã— 5490\t20â€¯m\n# B12\t5490 Ã— 5490\t20â€¯m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.072471Z","iopub.execute_input":"2026-02-06T09:49:34.072747Z","iopub.status.idle":"2026-02-06T09:49:34.082867Z","shell.execute_reply.started":"2026-02-06T09:49:34.072717Z","shell.execute_reply":"2026-02-06T09:49:34.082283Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Sub-sample 3k Data","metadata":{}},{"cell_type":"code","source":"# root_dir = \"/Volumes/WD_Rabina/competition/extracted_data/s2a\"\n# # List all folders\n# scenes = sorted(glob.glob(os.path.join(root_dir, \"*/\")))\n# # print(scenes)\n# print(len(scenes))\n\n# no_of_files=3000\n\n# # Randomly select 3000 scenes (without replacement)\n# selected_scenes = random.sample(scenes, k=3000)\n\n# print(f\"Total selected scenes: {len(selected_scenes)}\")\n# # print(selected_scenes[:10])  # show first 10 for sanity check\n\n# # Path to new folder where selected scenes will be copied\n# destination_root = \"data/s2a_3k_sample\"\n# os.makedirs(destination_root, exist_ok=True)  # create folder if it doesn't exist\n\n# # Copy each selected folder\n# count=0\n# for scene_path in selected_scenes:\n#     # Get folder name only (e.g., \"000015\")\n#     folder_name = os.path.basename(os.path.normpath(scene_path))\n    \n#     # Destination path\n#     dest_path = os.path.join(destination_root, folder_name)\n#     count=count+1\n#     print(count)\n#     # Copy folder and all its contents\n#     shutil.copytree(scene_path, dest_path)\n\n# print(f\"Copied {len(selected_scenes)} folders to {destination_root}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.083993Z","iopub.execute_input":"2026-02-06T09:49:34.084224Z","iopub.status.idle":"2026-02-06T09:49:34.092793Z","shell.execute_reply.started":"2026-02-06T09:49:34.084197Z","shell.execute_reply":"2026-02-06T09:49:34.092118Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Settings","metadata":{}},{"cell_type":"code","source":"target_size = 224\ntarget_batch_size=128 #prefer 256 or 128\ntarget_num_workers=4\ntarget_max_epoch=50\nuse_peft = True  \nfrom datetime import datetime\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nlogger = CSVLogger(\"logs\", name=f\"metrics_{timestamp}\")\n\n# logger = CSVLogger(\"logs\", name=f\"try_{timestamp}\")\n\n# Default\n# # Similar to SimCLR: https://arxiv.org/abs/2002.05709\n# aug1 = aug2 = K.AugmentationSequential(\n#     K.RandomResizedCrop(size=(size, size), scale=(0.2, 1)),\n#     K.RandomBrightness(brightness=(0.6, 1.4), p=0.8),\n#     K.RandomContrast(contrast=(0.6, 1.4), p=0.8),\n#     T.RandomGrayscale(weights=weights, p=0.2),\n#     K.RandomGaussianBlur(kernel_size=(ks, ks), sigma=(0.1, 2), p=0.5),\n#     K.RandomHorizontalFlip(),\n#     K.RandomVerticalFlip(),  # added\n#     data_keys=['input'],\n# )\n\naug = K.AugmentationSequential(\n    K.RandomResizedCrop(size=(target_size, target_size), scale=(0.4, 1.0)),\n    K.RandomHorizontalFlip(),\n    K.RandomVerticalFlip(),\n    K.RandomGaussianBlur(kernel_size=(7,7), sigma=(0.1, 1.5), p=0.3),\n    K.RandomBrightness(brightness=(0.85, 1.15), p=0.5),\n    data_keys=['input'],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.124692Z","iopub.execute_input":"2026-02-06T09:49:34.124933Z","iopub.status.idle":"2026-02-06T09:49:34.133366Z","shell.execute_reply.started":"2026-02-06T09:49:34.124910Z","shell.execute_reply":"2026-02-06T09:49:34.132859Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"code","source":"class SSLDataset(Dataset):\n    def __init__(self, scenes, bands, transforms=None):\n        \"\"\"\n        Args:\n            scenes (list): List of scene folder paths.\n            bands (list): List of band names (e.g., [\"B1\",\"B2\"]).\n            patch_size (tuple): Size of random crop (H, W).\n            transforms (callable, optional): Optional transform to apply to patches.\n        \"\"\"\n        self.scenes = scenes\n        self.bands = bands\n        # self.patch_size = patch_size\n        self.transforms = transforms\n        self.target_h= None\n        self.target_w = None\n        \n\n        # Precompute all timestamp paths to treat each timestamp as a sample\n        self.samples = []\n        for scene_path in scenes:\n            timestamps = sorted([\n                d for d in os.listdir(scene_path)\n                if os.path.isdir(os.path.join(scene_path, d))\n            ])\n            for ts in timestamps:\n                self.samples.append(os.path.join(scene_path, ts))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ts_path = self.samples[idx]\n\n        with rasterio.open(os.path.join(ts_path, \"B2.tif\")) as src:\n            target_h, target_w = src.height, src.width\n            # print(target_h,target_w, \"target width and height\" )\n\n        band_arrays = []\n\n        for b in self.bands:\n            path = os.path.join(ts_path, f\"{b}.tif\")\n            with rasterio.open(path) as src:\n                if src.height == target_h and src.width == target_w:\n                    arr = src.read(1).astype(np.float32)\n                else:\n                    arr = src.read(\n                        1,\n                        out_shape=(target_h, target_w),\n                        resampling=Resampling.bilinear\n                    ).astype(np.float32)\n\n            band_arrays.append(arr)\n\n        # Insert fake B10\n        insert_idx = 10\n        b10_pad = np.zeros((target_h, target_w), dtype=np.float32)\n        band_arrays.insert(insert_idx, b10_pad)\n\n        img = np.stack(band_arrays, axis=0)\n\n        # img_patch = self._random_crop(img)\n\n        patch_tensor = torch.tensor(img, dtype=torch.float32)\n\n        if self.transforms:\n            patch_tensor = self.transforms(patch_tensor)\n\n        return {\"image\": patch_tensor}\n\n# def calculate_stats(dataset, n_samples=500):\n#     mean = 0\n#     std = 0\n#     print(len(dataset))\n#     n = min(len(dataset), n_samples)\n\n#     for i in range(n):\n#         sample = dataset[i]\n#         img = sample[\"image\"]   # <-- key fix\n\n#         mean += img.mean(dim=(1, 2))\n#         std += img.std(dim=(1, 2))\n\n#     mean /= n\n#     std /= n\n\n#     return mean, std\n\ndef calculate_stats(dataset, n_samples=500):\n    mean = 0\n    std = 0\n    total = len(dataset)\n    n = min(total, n_samples)\n\n    # Randomly choose n indices\n    indices = np.random.choice(total, size=n, replace=False)\n    count=0\n    for i in indices:\n        count=count+1\n        print(count)\n        sample = dataset[i]\n        img = sample[\"image\"]   # TorchGeo-style dictionary\n\n        mean += img.mean(dim=(1, 2))\n        std += img.std(dim=(1, 2))\n\n    mean /= n\n    std /= n\n\n    return mean, std\n\n\ndef summary_trainable(model):\n    table = PrettyTable()\n    table.field_names = [\"Module\", \"Type\", \"Trainable Params\", \"Total Params\"]\n\n    for name, module in model.named_children():\n        total_params = sum(p.numel() for p in module.parameters())\n        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n        table.add_row([name, type(module).__name__, f\"{trainable_params:,}\", f\"{total_params:,}\"])\n\n    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n    \n    print(table)\n    print(f\"Total trainable parameters: {total_trainable:,} ({total_trainable / 1e6:.2f} M)\")\n    print(f\"Total parameters: {total_params:,} ({total_params / 1e6:.2f} M)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.135783Z","iopub.execute_input":"2026-02-06T09:49:34.136051Z","iopub.status.idle":"2026-02-06T09:49:34.149942Z","shell.execute_reply.started":"2026-02-06T09:49:34.136016Z","shell.execute_reply":"2026-02-06T09:49:34.149337Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# root_dir = \"data/s2a\"\nroot_dir = \"/kaggle/input/icpr-2026-competition-ssl-s2a-3k-subset/ICPR_SSL_S2A_3k_sample\"\n# List all folders\nscenes = sorted(glob.glob(os.path.join(root_dir, \"*/\")))\n# scenes = [\"data/s2a/000015\", \"data/s2a/000016\"]  # list of scene folders\nbands = [\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"B11\",\"B12\"]\n# print(scenes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:34.151157Z","iopub.execute_input":"2026-02-06T09:49:34.151612Z","iopub.status.idle":"2026-02-06T09:49:35.361190Z","shell.execute_reply.started":"2026-02-06T09:49:34.151570Z","shell.execute_reply":"2026-02-06T09:49:35.360432Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# import time\n# # One time run to get mean and std\n# temp_dataset = SSLDataset(scenes, bands)\n# start_time=time.time()\n# mean, std = calculate_stats(temp_dataset, n_samples=10000)\n# end_time=time.time()\n# print(f\"calculate_stats time: {(end_time-start_time)/60} min\")\n# print(mean)\n# print(std)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:49:35.362077Z","iopub.execute_input":"2026-02-06T09:49:35.362382Z","iopub.status.idle":"2026-02-06T09:49:35.365718Z","shell.execute_reply.started":"2026-02-06T09:49:35.362355Z","shell.execute_reply":"2026-02-06T09:49:35.365074Z"},"scrolled":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# based on 10k samples\nmean= [1333.8029, 1488.1448, 1745.9066, 1985.6210, 2322.0129, 2837.1787,\n        3065.8462, 3192.4492, 3225.1826, 3344.8479,    0.0000, 2683.2991,\n        2116.8357]\nstd = [384.9683, 472.5244, 497.7275, 590.9384, 578.0192, 641.7764, 699.6282,\n        752.0769, 709.3992, 752.4539,   0.0000, 568.3574, 542.2833]\n\n# based on 500 sample\n# mean= [1041.5322, 1224.2570, 1549.6492, 1815.5840, 2171.1243, 2729.5166,\n#         2990.2266, 3074.2515, 3162.9661, 3260.7983,    0.0000, 2969.8357,\n#         2335.3250]\n# std = [328.5996, 410.0965, 443.5781, 547.2238, 519.4624, 547.1485, 606.4136,\n#         649.6067, 621.7164, 687.0721,   0.0000, 574.0366, 560.9932]\n\n# mean = [2358.7412, 2402.7629, 2580.9255, 2614.2227, 3057.6877, 3578.1008,\n#         3796.8345, 3795.6868, 3947.5913, 4833.6362,    0.0000, 3379.1743,\n#         2666.4465]\n# std = [2994.4861, 2847.0354, 2542.9307, 2411.1196, 2399.0249, 2137.6804,\n#         2036.8357, 2042.7140, 1957.9615, 3559.4121,    0.0000, 1535.7960,\n#         1393.8278]\n\n# to avoid 0 std\nstd = [max(s, 1e-5) for s in std]   \n\n# define transform\ntransform = transforms.Compose([\n    transforms.Resize((target_size, target_size)),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:50:08.487015Z","iopub.execute_input":"2026-02-06T09:50:08.487571Z","iopub.status.idle":"2026-02-06T09:50:08.492607Z","shell.execute_reply.started":"2026-02-06T09:50:08.487543Z","shell.execute_reply":"2026-02-06T09:50:08.491863Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"dataset = SSLDataset(scenes, bands, transforms=transform)\nprint(len(dataset))\nprint(dataset[0]['image'].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:50:11.693549Z","iopub.execute_input":"2026-02-06T09:50:11.693865Z","iopub.status.idle":"2026-02-06T09:50:21.595475Z","shell.execute_reply.started":"2026-02-06T09:50:11.693834Z","shell.execute_reply":"2026-02-06T09:50:21.594708Z"}},"outputs":[{"name":"stdout","text":"15426\ntorch.Size([13, 224, 224])\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"data_loader = DataLoader(dataset, batch_size=target_batch_size, shuffle=True, num_workers=target_num_workers)\nnum_batches = len(data_loader)\nprint(\"Number of batches:\", num_batches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:50:21.596738Z","iopub.execute_input":"2026-02-06T09:50:21.597038Z","iopub.status.idle":"2026-02-06T09:50:21.601616Z","shell.execute_reply.started":"2026-02-06T09:50:21.597012Z","shell.execute_reply":"2026-02-06T09:50:21.600832Z"}},"outputs":[{"name":"stdout","text":"Number of batches: 121\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:50:21.602442Z","iopub.execute_input":"2026-02-06T09:50:21.602642Z","iopub.status.idle":"2026-02-06T09:50:21.649001Z","shell.execute_reply.started":"2026-02-06T09:50:21.602620Z","shell.execute_reply":"2026-02-06T09:50:21.648385Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"import time\ntask = MoCoTask(\n    model=\"resnet50\",      \n    weights= ResNet50_Weights.SENTINEL2_ALL_MOCO,\n    in_channels=13,       \n    version=2,             # MoCo v2\n    size=target_size,          \n    augmentation1=aug,\n    augmentation2=aug,\n    lr=1e-4,\n    memory_bank_size=2048,\n    temperature=0.15,\n    # gather_distributed=True\n    # patience=5\n)\n\n# # Load your checkpoint to resume task\n# ckpt_path = \"/kaggle/working/ssl_3k_ckpt_20260206_063623.ckpt\"\n# task = task.load_from_checkpoint(ckpt_path)\n\n# -----------------------------\n# PEFT / Full Fine-Tuning Logic\n# -----------------------------\nif use_peft:\n    print(\"Using PEFT: freezing backbone except last block, training projection head...\")\n    for name, param in task.backbone.named_parameters():\n        if \"layer4\" in name:      # optionally fine-tune last residual block\n            # print(\"Layer 4 trainable\")\n            param.requires_grad = True\n        else:\n            param.requires_grad = False\nelse:\n    print(\"Full fine-tuning: backbone and projection head trainable...\")\n    for param in task.backbone.parameters():\n        param.requires_grad = True\n\n# Momentum backbone always frozen\nfor param in task.backbone_momentum.parameters():\n    param.requires_grad = False\n\n# Projection head always trainable\nfor param in task.projection_head.parameters():\n    param.requires_grad = True\n\n\n\n\n# Example usage for your task\nsummary_trainable(task)\n\ntrainer = Trainer(\n    max_epochs=target_max_epoch,\n    enable_progress_bar=True, \n    log_every_n_steps=num_batches,\n    precision=32,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    logger=logger)\n\nstart_time=time.time()\ntrainer.fit(task, data_loader)\nend_time=time.time()\nprint(f\"Training time: {(end_time-start_time)/60} min\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T09:50:21.650229Z","iopub.execute_input":"2026-02-06T09:50:21.650464Z","iopub.status.idle":"2026-02-06T13:19:34.479202Z","shell.execute_reply.started":"2026-02-06T09:50:21.650440Z","shell.execute_reply":"2026-02-06T13:19:34.478276Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchgeo/trainers/moco.py:209: UserWarning: MoCo v2 only uses 2 layers in its projection head\n  warnings.warn('MoCo v2 only uses 2 layers in its projection head')\n","output_type":"stream"},{"name":"stdout","text":"Downloading: \"https://hf.co/torchgeo/resnet50_sentinel2_all_moco/resolve/da4f3c9dbe09272eb902f3b37f46635fa4726879/resnet50_sentinel2_all_moco-df8b932e.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_sentinel2_all_moco-df8b932e.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.1M/90.1M [00:01<00:00, 68.2MB/s]\nTrainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\nğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","output_type":"stream"},{"name":"stdout","text":"Using PEFT: freezing backbone except last block, training projection head...\n+--------------------------+------------------------+------------------+--------------+\n|          Module          |          Type          | Trainable Params | Total Params |\n+--------------------------+------------------------+------------------+--------------+\n|         backbone         |         ResNet         |    14,964,736    |  23,539,392  |\n|    backbone_momentum     |         ResNet         |        0         |  23,539,392  |\n|     projection_head      |   MoCoProjectionHead   |    26,222,848    |  26,222,848  |\n| projection_head_momentum |   MoCoProjectionHead   |        0         |  26,222,848  |\n|        criterion         |       NTXentLoss       |        0         |      0       |\n|      augmentation1       | AugmentationSequential |        0         |      0       |\n+--------------------------+------------------------+------------------+--------------+\nTotal trainable parameters: 41,187,584 (41.19 M)\nTotal parameters: 99,524,480 (99.52 M)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName                    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType                  \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\nâ”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\nâ”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ backbone                 â”‚ ResNet                 â”‚ 23.5 M â”‚ train â”‚     0 â”‚\nâ”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ backbone_momentum        â”‚ ResNet                 â”‚ 23.5 M â”‚ train â”‚     0 â”‚\nâ”‚\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0mâ”‚ projection_head          â”‚ MoCoProjectionHead     â”‚ 26.2 M â”‚ train â”‚     0 â”‚\nâ”‚\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0mâ”‚ projection_head_momentum â”‚ MoCoProjectionHead     â”‚ 26.2 M â”‚ train â”‚     0 â”‚\nâ”‚\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0mâ”‚ criterion                â”‚ NTXentLoss             â”‚      0 â”‚ train â”‚     0 â”‚\nâ”‚\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0mâ”‚ augmentation1            â”‚ AugmentationSequential â”‚      0 â”‚ train â”‚     0 â”‚\nâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                     </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\nâ”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ backbone                 â”‚ ResNet                 â”‚ 23.5 M â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ backbone_momentum        â”‚ ResNet                 â”‚ 23.5 M â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>â”‚ projection_head          â”‚ MoCoProjectionHead     â”‚ 26.2 M â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>â”‚ projection_head_momentum â”‚ MoCoProjectionHead     â”‚ 26.2 M â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>â”‚ criterion                â”‚ NTXentLoss             â”‚      0 â”‚ train â”‚     0 â”‚\nâ”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>â”‚ augmentation1            â”‚ AugmentationSequential â”‚      0 â”‚ train â”‚     0 â”‚\nâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 41.2 M                                                                                           \n\u001b[1mNon-trainable params\u001b[0m: 58.3 M                                                                                       \n\u001b[1mTotal params\u001b[0m: 99.5 M                                                                                               \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 398                                                                        \n\u001b[1mModules in train mode\u001b[0m: 460                                                                                         \n\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 41.2 M                                                                                           \n<span style=\"font-weight: bold\">Non-trainable params</span>: 58.3 M                                                                                       \n<span style=\"font-weight: bold\">Total params</span>: 99.5 M                                                                                               \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 398                                                                        \n<span style=\"font-weight: bold\">Modules in train mode</span>: 460                                                                                         \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cefd1f3d7ba640eeb3cfa26b490bfab6"}},"metadata":{}},{"name":"stderr","text":"`Trainer.fit` stopped: `max_epochs=50` reached.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"Training time: 209.16388917764027 min\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"print(task.trainer.logged_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T13:19:34.487800Z","iopub.execute_input":"2026-02-06T13:19:34.488121Z","iopub.status.idle":"2026-02-06T13:19:34.493857Z","shell.execute_reply.started":"2026-02-06T13:19:34.488092Z","shell.execute_reply":"2026-02-06T13:19:34.493233Z"}},"outputs":[{"name":"stdout","text":"{'train_ssl_std': tensor(0.0159), 'train_loss': tensor(3.2313)}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Save the backbone encoder only\ntorch.save(task.backbone.state_dict(),f\"ssl_backbone_{timestamp}.pth\")\n\ntorch.save(task.projection_head.state_dict(), f\"projection_head_{timestamp}.pth\")\ntrainer.save_checkpoint(f\"ssl_3k_ckpt_{timestamp}.ckpt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T13:19:34.494519Z","iopub.execute_input":"2026-02-06T13:19:34.494762Z","iopub.status.idle":"2026-02-06T13:19:37.622247Z","shell.execute_reply.started":"2026-02-06T13:19:34.494738Z","shell.execute_reply":"2026-02-06T13:19:37.621629Z"}},"outputs":[{"name":"stderr","text":"`weights_only` was not set, defaulting to `False`.\n","output_type":"stream"}],"execution_count":35}]}